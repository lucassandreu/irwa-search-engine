{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d943ceea",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# DELIVERY 2  \n",
    "## **Indexing and Evaluation**\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc77d9b4",
   "metadata": {},
   "source": [
    "## **PART 1: Indexing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b0adad",
   "metadata": {},
   "source": [
    "### **STEP 1 — Build inverted index:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029db64",
   "metadata": {},
   "source": [
    "### **Main Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "182a7618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading enriched dataset: C:\\Users\\Pol\\Documents\\POL\\UNI\\WEB\\irwa-search-engine\\data\\fashion_products_dataset_enriched.json\n",
      "Index will be saved in:   C:\\Users\\Pol\\Documents\\POL\\UNI\\WEB\\irwa-search-engine\\data\\index\n",
      "Loaded 28080 docs\n",
      "Vocabulary size: 9,048\n",
      "Saved inverted index to: C:\\Users\\Pol\\Documents\\POL\\UNI\\WEB\\irwa-search-engine\\data\\index\\boolean_inverted_index.json\n",
      "Saved doc map         to: C:\\Users\\Pol\\Documents\\POL\\UNI\\WEB\\irwa-search-engine\\data\\index\\docid_pid_map.json\n",
      "Saved fields          to: C:\\Users\\Pol\\Documents\\POL\\UNI\\WEB\\irwa-search-engine\\data\\index\\indexed_fields.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set, Any, Iterable\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "REPO_ROOT = NOTEBOOK_DIR.parents[1] if NOTEBOOK_DIR.name in {\"part_1\", \"part_2\"} else NOTEBOOK_DIR\n",
    "sys.path.append(str(REPO_ROOT / \"project_progress\"))\n",
    "from utils.preprocessing import preprocess_text_field\n",
    "\n",
    "\n",
    "\n",
    "# Path\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "REPO_ROOT = NOTEBOOK_DIR.parents[1]          \n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "INPUT = DATA_DIR / \"fashion_products_dataset_enriched.json\"\n",
    "\n",
    "INDEX_DIR = DATA_DIR / \"index\"\n",
    "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INDEX_FILE = INDEX_DIR / \"boolean_inverted_index.json\"\n",
    "DOCMAP_FILE = INDEX_DIR / \"docid_pid_map.json\"\n",
    "FIELDS_FILE = INDEX_DIR / \"indexed_fields.json\"\n",
    "\n",
    "print(f\"Reading enriched dataset: {INPUT}\")\n",
    "print(f\"Index will be saved in:   {INDEX_DIR}\")\n",
    "\n",
    "\n",
    "if not INPUT.exists():\n",
    "    raise FileNotFoundError(f\"Enriched dataset not found: {INPUT}\")\n",
    "docs: List[Dict[str, Any]] = json.loads(INPUT.read_text(encoding=\"utf-8\"))\n",
    "print(f\"Loaded {len(docs)} docs\")\n",
    "\n",
    "\n",
    "INDEXED_TEXT_FIELDS = [\n",
    "    \"title_clean\",\n",
    "    \"description_clean\",\n",
    "    \"metadata_clean\",   \n",
    "]\n",
    "\n",
    "\n",
    "# doc_id is an integer, stable order = index in list\n",
    "docid_to_pid: Dict[int, str] = {}\n",
    "pid_to_docid: Dict[str, int] = {}\n",
    "\n",
    "for i, r in enumerate(docs):\n",
    "    pid = r.get(\"pid\")\n",
    "    if not pid:\n",
    "        pid = r.get(\"_id\", f\"missing_pid_{i}\")\n",
    "    docid_to_pid[i] = pid\n",
    "    pid_to_docid[pid] = i\n",
    "\n",
    "def _doc_tokens(record: Dict[str, Any], fields: Iterable[str]) -> List[str]:\n",
    "    toks: List[str] = []\n",
    "    for f in fields:\n",
    "        val = record.get(f)\n",
    "        if not val:\n",
    "            continue\n",
    "        # We already have cleaned strings; just split.\n",
    "        toks.extend(str(val).split())\n",
    "    return toks\n",
    "\n",
    "\n",
    "# Build inverted index \n",
    "vocab: Dict[str, Set[int]] = defaultdict(set)\n",
    "\n",
    "for doc_id, rec in enumerate(docs):\n",
    "    tokens = _doc_tokens(rec, INDEXED_TEXT_FIELDS)\n",
    "    if not tokens:\n",
    "        continue\n",
    "    # Use unique terms per doc for Boolean presence posting\n",
    "    for term in set(tokens):\n",
    "        vocab[term].add(doc_id)\n",
    "\n",
    "# Convert sets to sorted lists for compactness and efficient AND intersections\n",
    "inverted_index: Dict[str, List[int]] = {t: sorted(list(s)) for t, s in vocab.items()}\n",
    "print(f\"Vocabulary size: {len(inverted_index):,}\")\n",
    "\n",
    "\n",
    "INDEX_FILE.write_text(json.dumps(inverted_index), encoding=\"utf-8\")\n",
    "DOCMAP_FILE.write_text(json.dumps({\"docid_to_pid\": docid_to_pid}, ensure_ascii=False), encoding=\"utf-8\")\n",
    "FIELDS_FILE.write_text(json.dumps({\"indexed_fields\": INDEXED_TEXT_FIELDS}, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Saved inverted index to: {INDEX_FILE}\")\n",
    "print(f\"Saved doc map         to: {DOCMAP_FILE}\")\n",
    "print(f\"Saved fields          to: {FIELDS_FILE}\")\n",
    "\n",
    "\n",
    "REQUIRED_OUTPUT_FIELDS = [\n",
    "    \"pid\", \"title\", \"description\", \"brand\", \"category\", \"sub_category\",\n",
    "    \"product_details\", \"seller\", \"out_of_stock\", \"selling_price\", \"discount\",\n",
    "    \"actual_price\", \"average_rating\", \"url\"\n",
    "]\n",
    "\n",
    "def _query_tokens(q: str) -> List[str]:\n",
    "    # Use the same normalization and stemming pipeline as Step 1\n",
    "    proc = preprocess_text_field(q or \"\")\n",
    "    return proc[\"tokens\"]\n",
    "\n",
    "def _intersect_sorted(a: List[int], b: List[int]) -> List[int]:\n",
    "    \"\"\"Intersect two sorted posting lists.\"\"\"\n",
    "    i=j=0\n",
    "    out: List[int] = []\n",
    "    while i < len(a) and j < len(b):\n",
    "        if a[i] == b[j]:\n",
    "            out.append(a[i])\n",
    "            i+=1; j+=1\n",
    "        elif a[i] < b[j]:\n",
    "            i+=1\n",
    "        else:\n",
    "            j+=1\n",
    "    return out\n",
    "\n",
    "def search_and(query: str, fields: List[str] = None, k: int = 20) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Conjunctive (AND) Boolean search.\n",
    "    Every returned doc must contain ALL query terms (after preprocessing).\n",
    "    Returns up to k full records with the required fields (when present).\n",
    "    \"\"\"\n",
    "    _ = fields  # kept for future extension; current index already built over INDEXED_TEXT_FIELDS\n",
    "    q_terms = _query_tokens(query)\n",
    "    if not q_terms:\n",
    "        return []\n",
    "\n",
    "    # Load postings lists; if any term not in vocab -> empty result\n",
    "    postings_lists: List[List[int]] = []\n",
    "    for t in q_terms:\n",
    "        p = inverted_index.get(t)\n",
    "        if not p:\n",
    "            return []\n",
    "        postings_lists.append(p)\n",
    "\n",
    "    # Intersect from shortest to longest for speed\n",
    "    postings_lists.sort(key=len)\n",
    "    result_ids = postings_lists[0]\n",
    "    for pl in postings_lists[1:]:\n",
    "        result_ids = _intersect_sorted(result_ids, pl)\n",
    "        if not result_ids:\n",
    "            break\n",
    "\n",
    "    # Map to records and keep only required output fields (when present)\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for did in result_ids[:k]:\n",
    "        rec = docs[did]\n",
    "        # Build a thin view with required fields (include only those present)\n",
    "        view = {f: rec.get(f) for f in REQUIRED_OUTPUT_FIELDS if f in rec}\n",
    "        # Always include pid\n",
    "        if \"pid\" not in view:\n",
    "            view[\"pid\"] = rec.get(\"pid\") or docid_to_pid.get(did)\n",
    "        out.append(view)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7122da5d",
   "metadata": {},
   "source": [
    "### **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f721aea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'women full sleeve sweatshirt cotton'  -> 5 hits (showing up to 5)\n",
      " - SWSFJY5ZBGZK49ZB | Full Sleeve Solid Women Sweatshirt\n",
      " - SWSFVEV2PZTNSVG6 | Full Sleeve Solid Women Sweatshirt\n",
      " - SWSFJY5ZRDUYGWN3 | Full Sleeve Solid Women Sweatshirt\n",
      " - SWSFJY5ZJZKVGUFT | Full Sleeve Striped Women Sweatshirt\n",
      " - SWSFJY5ZKQNGYZZJ | Full Sleeve Solid Women Sweatshirt\n",
      "\n",
      "Query: 'men slim jeans blue'  -> 5 hits (showing up to 5)\n",
      " - TSHEXH594YXPYNEQ | Solid Men Round Neck Black, Grey T-Shirt\n",
      " - TSHEXM9HRJ3JGCZX | Solid Men V-neck Blue T-Shirt\n",
      " - BZRFUXVMEZUPAFFB | Self Design Single Breasted Casual Men Full Sleeve Blazer  (Blue)\n",
      " - SHTFTGKHFSAUBWVC | Men Slim Fit Solid Spread Collar Casual Shirt  (Pack of 3)\n",
      " - SHTFTGKHG4YSFBXZ | Men Slim Fit Solid Button Down Collar Casual Shirt  (Pack of 3)\n"
     ]
    }
   ],
   "source": [
    "tests = [\n",
    "    \"women full sleeve sweatshirt cotton\",\n",
    "    \"men slim jeans blue\",\n",
    "]\n",
    "\n",
    "for q in tests:\n",
    "    hits = search_and(q, k=5)\n",
    "    print(f\"\\nQuery: {q!r}  -> {len(hits)} hits (showing up to 5)\")\n",
    "    for h in hits[:5]:\n",
    "        print(\" -\", h.get(\"pid\"), \"|\", (h.get(\"title\") or \"\")[:80])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irwa_venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
