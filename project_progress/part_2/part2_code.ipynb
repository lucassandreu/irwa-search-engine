{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d943ceea",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# **DELIVERY 2**\n",
    "## **Indexing and Evaluation**\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc77d9b4",
   "metadata": {},
   "source": [
    "## **PART 1: Indexing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b0adad",
   "metadata": {},
   "source": [
    "### **STEP 1 — Build inverted index:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029db64",
   "metadata": {},
   "source": [
    "### **Main Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "182a7618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading enriched dataset: C:\\Users\\Pol\\Documents\\POL\\UNI\\WEB\\irwa-search-engine\\data\\fashion_products_dataset_enriched.json\n",
      "Index will be saved in:   C:\\Users\\Pol\\Documents\\POL\\UNI\\WEB\\irwa-search-engine\\data\\index\n",
      "Loaded 28080 docs\n",
      "Vocabulary size: 9,048\n",
      "Saved inverted index to: C:\\Users\\Pol\\Documents\\POL\\UNI\\WEB\\irwa-search-engine\\data\\index\\boolean_inverted_index.json\n",
      "Saved doc map         to: C:\\Users\\Pol\\Documents\\POL\\UNI\\WEB\\irwa-search-engine\\data\\index\\docid_pid_map.json\n",
      "Saved fields          to: C:\\Users\\Pol\\Documents\\POL\\UNI\\WEB\\irwa-search-engine\\data\\index\\indexed_fields.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set, Any, Iterable\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "REPO_ROOT = NOTEBOOK_DIR.parents[1] if NOTEBOOK_DIR.name in {\"part_1\", \"part_2\"} else NOTEBOOK_DIR\n",
    "sys.path.append(str(REPO_ROOT / \"project_progress\"))\n",
    "from utils.preprocessing import preprocess_text_field\n",
    "\n",
    "\n",
    "\n",
    "# Path\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "REPO_ROOT = NOTEBOOK_DIR.parents[1]          \n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "INPUT = DATA_DIR / \"fashion_products_dataset_enriched.json\"\n",
    "\n",
    "INDEX_DIR = DATA_DIR / \"index\"\n",
    "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INDEX_FILE = INDEX_DIR / \"boolean_inverted_index.json\"\n",
    "DOCMAP_FILE = INDEX_DIR / \"docid_pid_map.json\"\n",
    "FIELDS_FILE = INDEX_DIR / \"indexed_fields.json\"\n",
    "\n",
    "print(f\"Reading enriched dataset: {INPUT}\")\n",
    "print(f\"Index will be saved in:   {INDEX_DIR}\")\n",
    "\n",
    "\n",
    "if not INPUT.exists():\n",
    "    raise FileNotFoundError(f\"Enriched dataset not found: {INPUT}\")\n",
    "docs: List[Dict[str, Any]] = json.loads(INPUT.read_text(encoding=\"utf-8\"))\n",
    "print(f\"Loaded {len(docs)} docs\")\n",
    "\n",
    "\n",
    "INDEXED_TEXT_FIELDS = [\n",
    "    \"title_clean\",\n",
    "    \"description_clean\",\n",
    "    \"metadata_clean\",   \n",
    "]\n",
    "\n",
    "\n",
    "# doc_id is an integer, stable order = index in list\n",
    "docid_to_pid: Dict[int, str] = {}\n",
    "pid_to_docid: Dict[str, int] = {}\n",
    "\n",
    "for i, r in enumerate(docs):\n",
    "    pid = r.get(\"pid\")\n",
    "    if not pid:\n",
    "        pid = r.get(\"_id\", f\"missing_pid_{i}\")\n",
    "    docid_to_pid[i] = pid\n",
    "    pid_to_docid[pid] = i\n",
    "\n",
    "def _doc_tokens(record: Dict[str, Any], fields: Iterable[str]) -> List[str]:\n",
    "    toks: List[str] = []\n",
    "    for f in fields:\n",
    "        val = record.get(f)\n",
    "        if not val:\n",
    "            continue\n",
    "        # We already have cleaned strings; just split.\n",
    "        toks.extend(str(val).split())\n",
    "    return toks\n",
    "\n",
    "\n",
    "# Build inverted index \n",
    "vocab: Dict[str, Set[int]] = defaultdict(set)\n",
    "\n",
    "for doc_id, rec in enumerate(docs):\n",
    "    tokens = _doc_tokens(rec, INDEXED_TEXT_FIELDS)\n",
    "    if not tokens:\n",
    "        continue\n",
    "    # Use unique terms per doc for Boolean presence posting\n",
    "    for term in set(tokens):\n",
    "        vocab[term].add(doc_id)\n",
    "\n",
    "# Convert sets to sorted lists for compactness and efficient AND intersections\n",
    "inverted_index: Dict[str, List[int]] = {t: sorted(list(s)) for t, s in vocab.items()}\n",
    "print(f\"Vocabulary size: {len(inverted_index):,}\")\n",
    "\n",
    "\n",
    "INDEX_FILE.write_text(json.dumps(inverted_index), encoding=\"utf-8\")\n",
    "DOCMAP_FILE.write_text(json.dumps({\"docid_to_pid\": docid_to_pid}, ensure_ascii=False), encoding=\"utf-8\")\n",
    "FIELDS_FILE.write_text(json.dumps({\"indexed_fields\": INDEXED_TEXT_FIELDS}, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Saved inverted index to: {INDEX_FILE}\")\n",
    "print(f\"Saved doc map         to: {DOCMAP_FILE}\")\n",
    "print(f\"Saved fields          to: {FIELDS_FILE}\")\n",
    "\n",
    "\n",
    "REQUIRED_OUTPUT_FIELDS = [\n",
    "    \"pid\", \"title\", \"description\", \"brand\", \"category\", \"sub_category\",\n",
    "    \"product_details\", \"seller\", \"out_of_stock\", \"selling_price\", \"discount\",\n",
    "    \"actual_price\", \"average_rating\", \"url\"\n",
    "]\n",
    "\n",
    "def _query_tokens(q: str) -> List[str]:\n",
    "    # Use the same normalization and stemming pipeline as Step 1\n",
    "    proc = preprocess_text_field(q or \"\")\n",
    "    return proc[\"tokens\"]\n",
    "\n",
    "def _intersect_sorted(a: List[int], b: List[int]) -> List[int]:\n",
    "    \"\"\"Intersect two sorted posting lists.\"\"\"\n",
    "    i=j=0\n",
    "    out: List[int] = []\n",
    "    while i < len(a) and j < len(b):\n",
    "        if a[i] == b[j]:\n",
    "            out.append(a[i])\n",
    "            i+=1; j+=1\n",
    "        elif a[i] < b[j]:\n",
    "            i+=1\n",
    "        else:\n",
    "            j+=1\n",
    "    return out\n",
    "\n",
    "def search_and(query: str, fields: List[str] = None, k: int = 20) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Conjunctive (AND) Boolean search.\n",
    "    Every returned doc must contain ALL query terms (after preprocessing).\n",
    "    Returns up to k full records with the required fields (when present).\n",
    "    \"\"\"\n",
    "    _ = fields  # kept for future extension; current index already built over INDEXED_TEXT_FIELDS\n",
    "    q_terms = _query_tokens(query)\n",
    "    if not q_terms:\n",
    "        return []\n",
    "\n",
    "    # Load postings lists; if any term not in vocab -> empty result\n",
    "    postings_lists: List[List[int]] = []\n",
    "    for t in q_terms:\n",
    "        p = inverted_index.get(t)\n",
    "        if not p:\n",
    "            return []\n",
    "        postings_lists.append(p)\n",
    "\n",
    "    # Intersect from shortest to longest for speed\n",
    "    postings_lists.sort(key=len)\n",
    "    result_ids = postings_lists[0]\n",
    "    for pl in postings_lists[1:]:\n",
    "        result_ids = _intersect_sorted(result_ids, pl)\n",
    "        if not result_ids:\n",
    "            break\n",
    "\n",
    "    # Map to records and keep only required output fields (when present)\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for did in result_ids[:k]:\n",
    "        rec = docs[did]\n",
    "        # Build a thin view with required fields (include only those present)\n",
    "        view = {f: rec.get(f) for f in REQUIRED_OUTPUT_FIELDS if f in rec}\n",
    "        # Always include pid\n",
    "        if \"pid\" not in view:\n",
    "            view[\"pid\"] = rec.get(\"pid\") or docid_to_pid.get(did)\n",
    "        out.append(view)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c20f01d",
   "metadata": {},
   "source": [
    "### **STEP 2 — Propose test queries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "016d3163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Proposed Test Queries (data-driven) ===\n",
      "1. men shirt regular blue  | tokens=['men', 'shirt', 'regular', 'blue'] | DF-score=53690 | matches≈50\n",
      "2. men shirt solid blue  | tokens=['men', 'shirt', 'solid', 'blue'] | DF-score=50331 | matches≈50\n",
      "3. women cotton kurta straight  | tokens=['women', 'cotton', 'kurta', 'straight'] | DF-score=35775 | matches≈50\n",
      "4. men slim fit formal shirt  | tokens=['men', 'slim', 'fit', 'formal', 'shirt'] | DF-score=57466 | matches≈50\n",
      "5. men running shoes black  | tokens=['men', 'run', 'shoe', 'black'] | DF-score=21967 | matches≈28\n",
      "\n",
      "Saved queries to: C:\\Users\\Pol\\Documents\\POL\\UNI\\WEB\\irwa-search-engine\\data\\index\\proposed_test_queries.json\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def df(term: str) -> int:\n",
    "    \"\"\"Document frequency of a term (0 if absent).\"\"\"\n",
    "    return len(inverted_index.get(term, []))\n",
    "\n",
    "def in_vocab(token: str) -> bool:\n",
    "    return token in inverted_index\n",
    "\n",
    "def stem_phrase(phrase: str) -> list[str]:\n",
    "    return preprocess_text_field(phrase)[\"tokens\"]\n",
    "\n",
    "def phrase_ok(phrase: str) -> bool:\n",
    "    \"\"\"All tokens exist in vocab AND the AND-query returns at least one result.\"\"\"\n",
    "    toks = stem_phrase(phrase)\n",
    "    if not toks or not all(in_vocab(t) for t in toks):\n",
    "        return False\n",
    "    return len(search_and(phrase, k=1)) > 0\n",
    "\n",
    "def term_popularity_score(tokens: list[str]) -> int:\n",
    "    \"\"\"Sum of dfs for quick 'popularity' proxy.\"\"\"\n",
    "    return sum(df(t) for t in tokens)\n",
    "\n",
    "# Candidate lexicons (raw phrases)\n",
    "gender_phrases  = [\"men\", \"women\"]\n",
    "type_phrases    = [\n",
    "    \"jeans\", \"shirt\", \"t shirt\", \"hoodie\", \"sweatshirt\",\n",
    "    \"track pants\", \"kurta\", \"dress\", \"jacket\"\n",
    "]\n",
    "color_phrases   = [\"black\", \"blue\", \"white\", \"grey\", \"red\", \"green\", \"pink\"]\n",
    "material_phrases= [\"cotton\", \"polyester\", \"denim\", \"linen\", \"silk\"]\n",
    "fit_phrases     = [\"slim\", \"skinny\", \"regular\", \"straight\", \"high waist\"]\n",
    "style_phrases   = [\"printed\", \"solid\", \"striped\", \"floral\"]\n",
    "\n",
    "# Keep only phrases whose tokens exist in vocab (post-stemming)\n",
    "def filter_vocab(phrases):\n",
    "    good = []\n",
    "    for p in phrases:\n",
    "        toks = stem_phrase(p)\n",
    "        if toks and all(in_vocab(t) for t in toks):\n",
    "            good.append((p, toks, term_popularity_score(toks)))\n",
    "    # Sort by popularity descending (PRP)\n",
    "    return sorted(good, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "gender_ok   = filter_vocab(gender_phrases)\n",
    "types_ok    = filter_vocab(type_phrases)\n",
    "colors_ok   = filter_vocab(color_phrases)\n",
    "materials_ok= filter_vocab(material_phrases)\n",
    "fits_ok     = filter_vocab(fit_phrases)\n",
    "styles_ok   = filter_vocab(style_phrases)\n",
    "\n",
    "# Compose candidate query templates: (gender) + (type) + (attribute sets)\n",
    "templates = [\n",
    "    [\"{g}\", \"{m}\", \"{t}\", \"{c}\"],                 # gender + material + type + color\n",
    "    [\"{g}\", \"{t}\", \"{f}\", \"{c}\"],                 # gender + type + fit + color\n",
    "    [\"{g}\", \"full sleeve\", \"{t}\", \"{m}\"],         # gender + sleeve attr + type + material\n",
    "    [\"{g}\", \"{t}\", \"{s}\", \"{c}\"],                 # gender + type + style + color\n",
    "    [\"{g}\", \"high waist\", \"{t}\", \"{c}\"],          # gender + high waist + type + color\n",
    "]\n",
    "\n",
    "def pick(pop_list, k=1):\n",
    "    return [p[0] for p in pop_list[:k]] if pop_list else []\n",
    "\n",
    "# Generate diverse, popular queries that actually return hits\n",
    "proposed = []\n",
    "attempts = 0\n",
    "seen_main_types = set()\n",
    "\n",
    "while len(proposed) < 5 and attempts < 200:\n",
    "    attempts += 1\n",
    "    tpl = random.choice(templates)\n",
    "\n",
    "    g = pick(gender_ok, 1) or [\"women\"]\n",
    "    t = pick(types_ok, 1) or [\"jeans\"]\n",
    "    m = pick(materials_ok, 1) or [\"cotton\"]\n",
    "    c = pick(colors_ok, 1) or [\"blue\"]\n",
    "    f = pick(fits_ok, 1) or [\"slim\"]\n",
    "    s = pick(styles_ok, 1) or [\"printed\"]\n",
    "\n",
    "    phrase = \" \".join(\n",
    "        x.format(g=g[0], t=t[0], m=m[0], c=c[0], f=f[0], s=s[0])\n",
    "        for x in tpl\n",
    "    )\n",
    "    phrase = \" \".join(phrase.split())  # clean double spaces\n",
    "\n",
    "    # Require the AND query to return hits and encourage diversity by not repeating the same main type too much.\n",
    "    main_type = t[0]\n",
    "    if phrase_ok(phrase):\n",
    "        if sum(1 for q in proposed if main_type in q) < 2:\n",
    "            proposed.append(phrase)\n",
    "\n",
    "# Fallbacks (just in case)\n",
    "fallbacks = [\n",
    "    \"women cotton kurta straight\",\n",
    "    \"men slim fit formal shirt\",\n",
    "    \"women high waist blue jeans\",\n",
    "    \"men running shoes black\",\n",
    "    \"women printed dress floral\"\n",
    "]\n",
    "for fb in fallbacks:\n",
    "    if len(proposed) >= 5:\n",
    "        break\n",
    "    if fb not in proposed and phrase_ok(fb):\n",
    "        proposed.append(fb)\n",
    "\n",
    "# Deduplicate and trim to 5\n",
    "seen = set()\n",
    "unique_proposed = []\n",
    "for q in proposed:\n",
    "    if q not in seen:\n",
    "        unique_proposed.append(q)\n",
    "        seen.add(q)\n",
    "proposed = unique_proposed[:5]\n",
    "\n",
    "print(\"=== Proposed Test Queries (data-driven) ===\")\n",
    "for i, q in enumerate(proposed, 1):\n",
    "    hits = len(search_and(q, k=50))\n",
    "    toks = stem_phrase(q)\n",
    "    score = term_popularity_score(toks)\n",
    "    print(f\"{i}. {q}  | tokens={toks} | DF-score={score} | matches≈{hits}\")\n",
    "\n",
    "# Store for later evaluation/report\n",
    "TEST_QUERIES_FILE = DATA_DIR / \"index\" / \"proposed_test_queries.json\"\n",
    "TEST_QUERIES_FILE.write_text(json.dumps({\"queries\": proposed}, indent=2), encoding=\"utf-8\")\n",
    "print(f\"\\nSaved queries to: {TEST_QUERIES_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9279b8",
   "metadata": {},
   "source": [
    "### **STEP 3 — Ranking our results:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66877920",
   "metadata": {},
   "source": [
    "### **Main Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "411254d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "# Configuration\n",
    "INDEXED_TEXT_FIELDS = [\"title_clean\", \"description_clean\", \"metadata_clean\"]\n",
    "FIELD_WEIGHTS: Dict[str, float] = {\n",
    "    \"title_clean\": 2.0,     \n",
    "    \"description_clean\": 1.0,  \n",
    "    \"metadata_clean\": 0.7,     \n",
    "}\n",
    "\n",
    "# Utilities \n",
    "def _tokens_from_fields(record: Dict[str, Any], fields: Iterable[str]) -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Return per-field raw term frequencies:\n",
    "        { field_name : { term : count_in_that_field } }\n",
    "    \"\"\"\n",
    "    per_field_counts: Dict[str, Dict[str, int]] = {}\n",
    "    for f in fields:\n",
    "        txt = record.get(f)\n",
    "        if not txt:\n",
    "            continue\n",
    "        counts: Dict[str, int] = defaultdict(int)\n",
    "        for t in str(txt).split():\n",
    "            counts[t] += 1\n",
    "        if counts:\n",
    "            per_field_counts[f] = dict(counts)\n",
    "    return per_field_counts\n",
    "\n",
    "def _tf_log2(freq: float) -> float:\n",
    "    \"\"\"1 + log2(freq) if freq>0 else 0.\"\"\"\n",
    "    if freq <= 0:\n",
    "        return 0.0\n",
    "    return 1.0 + math.log(freq, 2)\n",
    "\n",
    "def _idf_log2(df_i: int, N: int) -> float:\n",
    "    \"\"\"idf = log2(N / df_i); assumes df_i >= 1.\"\"\"\n",
    "    if df_i <= 0 or N <= 0:\n",
    "        return 0.0\n",
    "    return math.log(N / df_i, 2)\n",
    "\n",
    "# Build TF postings & df\n",
    "N = len(docs)\n",
    "\n",
    "# term -> { doc_id : (field-weighted raw frequency) }\n",
    "tf_postings: Dict[str, Dict[int, float]] = defaultdict(dict)\n",
    "# term -> document frequency\n",
    "df_counts: Dict[str, int] = defaultdict(int)\n",
    "\n",
    "for doc_id, rec in enumerate(docs):\n",
    "    per_field = _tokens_from_fields(rec, INDEXED_TEXT_FIELDS)\n",
    "\n",
    "    term_freq_weighted: Dict[str, float] = defaultdict(float)\n",
    "    for field_name, counts in per_field.items():\n",
    "        w_f = FIELD_WEIGHTS.get(field_name, 1.0)\n",
    "        for term, f_ij_f in counts.items():\n",
    "            term_freq_weighted[term] += w_f * f_ij_f\n",
    "\n",
    "    for term, f_ij in term_freq_weighted.items():\n",
    "        tf_postings[term][doc_id] = f_ij\n",
    "\n",
    "# df_i = number of docs where term appears\n",
    "for term, posting in tf_postings.items():\n",
    "    df_counts[term] = len(posting)\n",
    "\n",
    "# Precompute document norms\n",
    "doc_norms: List[float] = [0.0] * N\n",
    "for term, posting in tf_postings.items():\n",
    "    idf_i = _idf_log2(df_counts[term], N)\n",
    "    if idf_i == 0.0:\n",
    "        continue\n",
    "    for d_id, f_ij in posting.items():\n",
    "        w_dt = _tf_log2(f_ij) * idf_i\n",
    "        if w_dt != 0.0:\n",
    "            doc_norms[d_id] += w_dt * w_dt\n",
    "\n",
    "doc_norms = [math.sqrt(v) if v > 0 else 0.0 for v in doc_norms]\n",
    "\n",
    "# Ranked search\n",
    "def search_tfidf(query: str, k: int = 20) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Rank documents by cosine similarity with TF-IDF (base-2 logs, no smoothing).\n",
    "    Returns top-k with 'score' plus the required output fields.\n",
    "    \"\"\"\n",
    "    q_proc = preprocess_text_field(query or \"\")\n",
    "    q_terms = q_proc[\"tokens\"]\n",
    "    if not q_terms:\n",
    "        return []\n",
    "\n",
    "    # query term frequencies\n",
    "    q_tf: Dict[str, int] = defaultdict(int)\n",
    "    for t in q_terms:\n",
    "        q_tf[t] += 1\n",
    "\n",
    "    # build query vector\n",
    "    q_weights: Dict[str, float] = {}\n",
    "    q_norm_sq = 0.0\n",
    "    for t, f_q in q_tf.items():\n",
    "        df_i = df_counts.get(t, 0)\n",
    "        if df_i <= 0:\n",
    "            continue  # unseen term\n",
    "        w_t = _tf_log2(f_q) * _idf_log2(df_i, N)\n",
    "        if w_t == 0.0:\n",
    "            continue\n",
    "        q_weights[t] = w_t\n",
    "        q_norm_sq += w_t * w_t\n",
    "\n",
    "    q_norm = math.sqrt(q_norm_sq) if q_norm_sq > 0 else 0.0\n",
    "    if q_norm == 0.0:\n",
    "        return []\n",
    "\n",
    "    # sparse dot product over postings of query terms\n",
    "    scores: Dict[int, float] = defaultdict(float)\n",
    "    for t, w_t in q_weights.items():\n",
    "        posting = tf_postings.get(t)\n",
    "        if not posting:\n",
    "            continue\n",
    "        idf_i = _idf_log2(df_counts[t], N)\n",
    "        if idf_i == 0.0:\n",
    "            continue\n",
    "        for d_id, f_ij in posting.items():\n",
    "            w_dt = _tf_log2(f_ij) * idf_i\n",
    "            if w_dt != 0.0:\n",
    "                scores[d_id] += w_t * w_dt\n",
    "\n",
    "    # cosine normalization and rank\n",
    "    ranked: List[Tuple[int, float]] = []\n",
    "    for d_id, dot in scores.items():\n",
    "        denom = doc_norms[d_id] * q_norm\n",
    "        if denom > 0:\n",
    "            ranked.append((d_id, dot / denom))\n",
    "    ranked.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for d_id, sc in ranked[:k]:\n",
    "        rec = docs[d_id]\n",
    "        view = {f: rec.get(f) for f in REQUIRED_OUTPUT_FIELDS if f in rec}\n",
    "        if \"pid\" not in view:\n",
    "            view[\"pid\"] = rec.get(\"pid\")\n",
    "        view[\"score\"] = float(sc)\n",
    "        results.append(view)\n",
    "    return results\n",
    "\n",
    "# AND-filtered TF-IDF\n",
    "def search_tfidf_and(query: str, k: int = 20) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Conjunctive AND filter first (Boolean), then TF-IDF rank within the survivors.\n",
    "    Helpful if your teacher wants AND semantics even for ranking.\n",
    "    \"\"\"\n",
    "    # Candidate set via Boolean AND\n",
    "    cand = search_and(query, k=10_000)\n",
    "    if not cand:\n",
    "        return []\n",
    "\n",
    "    cand_pids = {r[\"pid\"] for r in cand if r.get(\"pid\")}\n",
    "    cand_ids = {i for i, r in enumerate(docs) if r.get(\"pid\") in cand_pids}\n",
    "\n",
    "    # Build query vector (same as search_tfidf)\n",
    "    q_proc = preprocess_text_field(query or \"\")\n",
    "    q_terms = q_proc[\"tokens\"]\n",
    "    if not q_terms:\n",
    "        return []\n",
    "\n",
    "    q_tf: Dict[str, int] = defaultdict(int)\n",
    "    for t in q_terms:\n",
    "        q_tf[t] += 1\n",
    "\n",
    "    q_weights: Dict[str, float] = {}\n",
    "    q_norm_sq = 0.0\n",
    "    for t, f_q in q_tf.items():\n",
    "        df_i = df_counts.get(t, 0)\n",
    "        if df_i <= 0:\n",
    "            continue\n",
    "        w_t = _tf_log2(f_q) * _idf_log2(df_i, N)\n",
    "        if w_t == 0.0:\n",
    "            continue\n",
    "        q_weights[t] = w_t\n",
    "        q_norm_sq += w_t * w_t\n",
    "\n",
    "    q_norm = math.sqrt(q_norm_sq) if q_norm_sq > 0 else 0.0\n",
    "    if q_norm == 0.0:\n",
    "        return []\n",
    "\n",
    "    scores: Dict[int, float] = defaultdict(float)\n",
    "    for t, w_t in q_weights.items():\n",
    "        posting = tf_postings.get(t)\n",
    "        if not posting:\n",
    "            continue\n",
    "        idf_i = _idf_log2(df_counts[t], N)\n",
    "        if idf_i == 0.0:\n",
    "            continue\n",
    "        for d_id, f_ij in posting.items():\n",
    "            if d_id not in cand_ids:\n",
    "                continue\n",
    "            w_dt = _tf_log2(f_ij) * idf_i\n",
    "            if w_dt != 0.0:\n",
    "                scores[d_id] += w_t * w_dt\n",
    "\n",
    "    ranked: List[Tuple[int, float]] = []\n",
    "    for d_id, dot in scores.items():\n",
    "        denom = doc_norms[d_id] * q_norm\n",
    "        if denom > 0:\n",
    "            ranked.append((d_id, dot / denom))\n",
    "    ranked.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for d_id, sc in ranked[:k]:\n",
    "        rec = docs[d_id]\n",
    "        view = {f: rec.get(f) for f in REQUIRED_OUTPUT_FIELDS if f in rec}\n",
    "        if \"pid\" not in view:\n",
    "            view[\"pid\"] = rec.get(\"pid\")\n",
    "        view[\"score\"] = float(sc)\n",
    "        results.append(view)\n",
    "    return results\n",
    "\n",
    "# Persist ranked results\n",
    "def save_ranked_results(out_path: Path, queries: Dict[str, List[str]], use_and_filter: bool = False, k: int = 20) -> Path:\n",
    "    \"\"\"\n",
    "    Save ranked results for groups of queries.\n",
    "    queries = {\"provided\": [q1, q2, ...], \"proposed\": [q3, ...]}\n",
    "    \"\"\"\n",
    "    out = {\"provided_queries\": {}, \"proposed_queries\": {}}\n",
    "    ranker = search_tfidf_and if use_and_filter else search_tfidf\n",
    "\n",
    "    for group, qlist in queries.items():\n",
    "        for q in qlist:\n",
    "            out_key = \"provided_queries\" if group == \"provided\" else \"proposed_queries\"\n",
    "            out[out_key][q] = ranker(q, k=k)\n",
    "\n",
    "    out_path.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5d383",
   "metadata": {},
   "source": [
    "### **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba56b182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF (log2) top for: 'women full sleeve sweatshirt cotton'\n",
      "  0.9074 | SWSFZVTTQCB4SJ7F | Full Sleeve Solid Women Sweatshirt\n",
      "  0.8760 | SWSFQGS456JAZCQB | Full Sleeve Printed Women Sweatshirt\n",
      "  0.8724 | SWSFYTYMNTBNARUN | Full Sleeve Solid Women Sweatshirt\n",
      "\n",
      "TF-IDF (log2) top for: 'men slim jeans blue'\n",
      "  0.7168 | JEAFSKYHZHSZZC9S | Slim Men Blue Jeans\n",
      "  0.7147 | JEAFRAQXEKGUPNUN | Slim Men Blue Jeans\n",
      "  0.7096 | JEAFQF6JBUSEXHVF | Slim Men Blue Jeans\n",
      "\n",
      "Ranked results saved to: C:\\Users\\Pol\\Documents\\POL\\UNI\\WEB\\irwa-search-engine\\data\\index\\ranked_results.json\n"
     ]
    }
   ],
   "source": [
    "# Quick demo on course queries\n",
    "for q in [\"women full sleeve sweatshirt cotton\", \"men slim jeans blue\"]:\n",
    "    top = search_tfidf(q, k=3)\n",
    "    print(f\"\\nTF-IDF (log2) top for: {q!r}\")\n",
    "    for r in top:\n",
    "        print(f\"  {r['score']:.4f} | {r.get('pid')} | {(r.get('title') or '')[:80]}\")\n",
    "\n",
    "# Save ranked results for report/repro\n",
    "RANKED_OUT = (DATA_DIR / \"index\" / \"ranked_results.json\")\n",
    "queries_for_report = {\n",
    "    \"provided\": [\n",
    "        \"women full sleeve sweatshirt cotton\",\n",
    "        \"men slim jeans blue\",\n",
    "    ],\n",
    "    # Optionally load your proposed queries file if it exists\n",
    "}\n",
    "pq_file = DATA_DIR / \"index\" / \"proposed_test_queries.json\"\n",
    "if pq_file.exists():\n",
    "    try:\n",
    "        queries_for_report[\"proposed\"] = json.loads(pq_file.read_text(encoding=\"utf-8\"))[\"queries\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "out_path = save_ranked_results(RANKED_OUT, queries_for_report, use_and_filter=False, k=20)\n",
    "print(f\"\\nRanked results saved to: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab69b52",
   "metadata": {},
   "source": [
    "## **PART 2: Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846babf1",
   "metadata": {},
   "source": [
    "### **STEP 1 — Implementing Metrics:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eb49201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "def precision_at_k(rel_ranked: List[int], k: int) -> float:\n",
    "    \"\"\"P@K = (# relevant in top-K) / K.\"\"\"\n",
    "    if k <= 0:\n",
    "        return 0.0\n",
    "    top = rel_ranked[:k]\n",
    "    return sum(top) / k\n",
    "\n",
    "def recall_at_k(rel_ranked: List[int], k: int, total_relevant: int) -> float:\n",
    "    \"\"\"R@K = (# relevant in top-K) / (# relevant in the corpus for this query).\"\"\"\n",
    "    if total_relevant <= 0:\n",
    "        return 0.0\n",
    "    return sum(rel_ranked[:k]) / total_relevant\n",
    "\n",
    "def average_precision_at_k(rel_ranked: List[int], k: int, total_relevant: int) -> float:\n",
    "    \"\"\"\n",
    "    AP@K = Average of precisions at ranks where a relevant item occurs, up to K,\n",
    "           divided by the TOTAL number of relevant documents for the query.\n",
    "    If there are no relevant docs, AP is 0 by convention.\n",
    "    \"\"\"\n",
    "    if total_relevant <= 0:\n",
    "        return 0.0\n",
    "    ap_sum = 0.0\n",
    "    for i in range(1, min(k, len(rel_ranked)) + 1):\n",
    "        if rel_ranked[i - 1] == 1:\n",
    "            ap_sum += precision_at_k(rel_ranked, i)\n",
    "    return ap_sum / total_relevant\n",
    "\n",
    "def f1_at_k(rel_ranked: List[int], k: int, total_relevant: int) -> float:\n",
    "    \"\"\"F1@K = 2 * P@K * R@K / (P@K + R@K).\"\"\"\n",
    "    p = precision_at_k(rel_ranked, k)\n",
    "    r = recall_at_k(rel_ranked, k, total_relevant)\n",
    "    if p + r == 0:\n",
    "        return 0.0\n",
    "    return 2.0 * p * r / (p + r)\n",
    "\n",
    "def mean_average_precision(all_ap: List[float]) -> float:\n",
    "    \"\"\"MAP = mean(AP) over queries.\"\"\"\n",
    "    return sum(all_ap) / len(all_ap) if all_ap else 0.0\n",
    "\n",
    "def mean_reciprocal_rank(all_rel_ranked: List[List[int]]) -> float:\n",
    "    \"\"\"\n",
    "    MRR = average over queries of 1/rank_of_first_relevant (rank is 1-based).\n",
    "    If a query has no relevant in its ranking, its contribution is 0.\n",
    "    \"\"\"\n",
    "    rr_sum = 0.0\n",
    "    for rels in all_rel_ranked:\n",
    "        rr = 0.0\n",
    "        for i, r in enumerate(rels, start=1):\n",
    "            if r == 1:\n",
    "                rr = 1.0 / i\n",
    "                break\n",
    "        rr_sum += rr\n",
    "    return rr_sum / len(all_rel_ranked) if all_rel_ranked else 0.0\n",
    "\n",
    "def dcg_at_k(rel_ranked: List[int], k: int) -> float:\n",
    "    \"\"\"\n",
    "    DCG@K with gains = 2^rel - 1 and log2 discount (positions 1-based):\n",
    "      DCG@K = sum_{i=1..K} (2^{rel_i} - 1) / log2(i + 1)\n",
    "    \"\"\"\n",
    "    import math\n",
    "    if k <= 0:\n",
    "        return 0.0\n",
    "    dcg = 0.0\n",
    "    upto = min(k, len(rel_ranked))\n",
    "    for i in range(1, upto + 1):\n",
    "        rel = rel_ranked[i - 1]\n",
    "        gain = (2 ** rel) - 1\n",
    "        dcg += gain / math.log2(i + 1)\n",
    "    return dcg\n",
    "\n",
    "def ndcg_at_k(rel_ranked: List[int], k: int) -> float:\n",
    "    \"\"\"NDCG@K = DCG@K / IDCG@K, where IDCG is DCG of rels sorted descending.\"\"\"\n",
    "    ideal = sorted(rel_ranked, reverse=True)\n",
    "    dcg = dcg_at_k(rel_ranked, k)\n",
    "    idcg = dcg_at_k(ideal, k)\n",
    "    if idcg == 0.0:\n",
    "        return 0.0\n",
    "    return dcg / idcg\n",
    "\n",
    "# Helpers to integrate with system \n",
    "def build_rel_vector(ranked_pids: List[str], gt_for_q: Dict[str, int]) -> Tuple[List[int], int]:\n",
    "    \"\"\"\n",
    "    Convert a ranked list of pids into a relevance vector and compute total relevant.\n",
    "    Inputs:\n",
    "      - ranked_pids: list of doc ids (pids) in the returned order of your system.\n",
    "      - gt_for_q: dict mapping pid -> 1/0 (ground truth relevance for ONE query).\n",
    "    Returns:\n",
    "      - rel_ranked: list[int], each element 1 if pid is relevant else 0 (aligned with ranked_pids)\n",
    "      - total_relevant: total number of relevant docs for this query in the corpus (sum(gt_for_q))\n",
    "    \"\"\"\n",
    "    rel_ranked = [int(gt_for_q.get(pid, 0)) for pid in ranked_pids]\n",
    "    total_relevant = sum(1 for v in gt_for_q.values() if v == 1)\n",
    "    return rel_ranked, total_relevant\n",
    "\n",
    "def evaluate_query_at_k(ranked_pids: List[str], gt_rels: Dict[str, int], k: int = 20) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate precision, recall, AP, F1, NDCG, MRR for a ranked list.\n",
    "    gt_rels: dict {pid: 1 or 0}, where 1 means relevant.\n",
    "    \"\"\"\n",
    "    rel_pids = {pid for pid, lbl in gt_rels.items() if lbl > 0}\n",
    "    retrieved = ranked_pids[:k]\n",
    "    retrieved_rels = [1 if pid in rel_pids else 0 for pid in retrieved]\n",
    "\n",
    "    # count relevant retrieved (overlap)\n",
    "    rel_retrieved = sum(retrieved_rels)\n",
    "    total_relevant = len(rel_pids)\n",
    "\n",
    "    precision = rel_retrieved / k if k > 0 else 0.0\n",
    "    recall = rel_retrieved / total_relevant if total_relevant > 0 else 0.0\n",
    "\n",
    "    # Average Precision (using 1 + log2 weighting)\n",
    "    precisions = []\n",
    "    num_rel_so_far = 0\n",
    "    for i, rel in enumerate(retrieved_rels, start=1):\n",
    "        if rel:\n",
    "            num_rel_so_far += 1\n",
    "            precisions.append(num_rel_so_far / i)\n",
    "    ap = sum(precisions) / total_relevant if total_relevant > 0 else 0.0\n",
    "\n",
    "    # F1-score\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "\n",
    "    # Reciprocal rank\n",
    "    rr = 0.0\n",
    "    for i, rel in enumerate(retrieved_rels, start=1):\n",
    "        if rel:\n",
    "            rr = 1.0 / i\n",
    "            break\n",
    "\n",
    "    # NDCG@K\n",
    "    def dcg(rels):\n",
    "        return sum((2 ** r - 1) / math.log2(i + 2) for i, r in enumerate(rels))\n",
    "    dcg_val = dcg(retrieved_rels)\n",
    "    ideal_rels = sorted(retrieved_rels, reverse=True)\n",
    "    idcg_val = dcg(ideal_rels)\n",
    "    ndcg = dcg_val / idcg_val if idcg_val > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        f\"P@{k}\": precision,\n",
    "        f\"R@{k}\": recall,\n",
    "        f\"AP@{k}\": ap,\n",
    "        f\"F1@{k}\": f1,\n",
    "        f\"NDCG@{k}\": ndcg,\n",
    "        \"MRR\": rr,\n",
    "        \"total_relevant\": total_relevant,\n",
    "        \"returned\": len(retrieved)\n",
    "    }\n",
    "\n",
    "def evaluate_multiple_at_k(\n",
    "    results_by_qid: Dict[str, List[str]],\n",
    "    gt_all: Dict[str, Dict[str, int]],\n",
    "    k: int = 20\n",
    ") -> Dict[str, Any]:\n",
    "    \n",
    "    \"\"\"Evaluate multiple queries at cut-off K.\"\"\"\n",
    "    per_query: Dict[str, Dict[str, float]] = {}\n",
    "    ap_list: List[float] = []\n",
    "    rel_lists: List[List[int]] = []\n",
    "    ndcgs: List[float] = []\n",
    "\n",
    "    for qid, ranked_pids in results_by_qid.items():\n",
    "        gt_for_q = gt_all.get(qid, {})\n",
    "        rel_ranked, total_rel = build_rel_vector(ranked_pids, gt_for_q)\n",
    "\n",
    "        m = {\n",
    "            f\"P@{k}\": precision_at_k(rel_ranked, k),\n",
    "            f\"R@{k}\": recall_at_k(rel_ranked, k, total_rel),\n",
    "            f\"AP@{k}\": average_precision_at_k(rel_ranked, k, total_rel),\n",
    "            f\"F1@{k}\": f1_at_k(rel_ranked, k, total_rel),\n",
    "            f\"NDCG@{k}\": ndcg_at_k(rel_ranked, k),\n",
    "            \"MRR\": mean_reciprocal_rank([rel_ranked]),\n",
    "            \"total_relevant\": total_rel,\n",
    "            \"returned\": len(ranked_pids),\n",
    "        }\n",
    "        per_query[qid] = m\n",
    "        ap_list.append(m[f\"AP@{k}\"])\n",
    "        rel_lists.append(rel_ranked)\n",
    "        ndcgs.append(m[f\"NDCG@{k}\"])\n",
    "\n",
    "    summary = {\n",
    "        \"K\": k,\n",
    "        \"MAP\": mean_average_precision(ap_list),\n",
    "        \"MRR\": mean_reciprocal_rank(rel_lists),\n",
    "        f\"mean_NDCG@{k}\": (sum(ndcgs) / len(ndcgs) if ndcgs else 0.0),\n",
    "    }\n",
    "    return {\"summary\": summary, \"per_query\": per_query}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f12df",
   "metadata": {},
   "source": [
    "### **STEP 2 — Applying the Evaluation Metrics:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9b62bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Config\n",
    "K = 20\n",
    "VAL_PATH = DATA_DIR / \"validation_labels.csv\"\n",
    "\n",
    "# Load ranked list (as appears in CSV order) and labels\n",
    "ranked_by_qid = defaultdict(list)\n",
    "gt_by_qid = defaultdict(dict)\n",
    "\n",
    "if not VAL_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Add validation_labels.csv to data/ directory for evaluation. File not found: {VAL_PATH}\")\n",
    "\n",
    "with VAL_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        qid = str(row[\"query_id\"]).strip()\n",
    "        pid = str(row[\"pid\"]).strip()\n",
    "        lab = int(row[\"labels\"])  # 1/0\n",
    "        ranked_by_qid[qid].append(pid)  \n",
    "        gt_by_qid[qid][pid] = lab       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f66874d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"query_1\": {\n",
      "    \"P@20\": 0.65,\n",
      "    \"R@20\": 1.0,\n",
      "    \"AP@20\": 0.694,\n",
      "    \"F1@20\": 0.788,\n",
      "    \"NDCG@20\": 0.873,\n",
      "    \"MRR\": 1.0,\n",
      "    \"total_relevant\": 13,\n",
      "    \"returned\": 20\n",
      "  },\n",
      "  \"query_2\": {\n",
      "    \"P@20\": 0.5,\n",
      "    \"R@20\": 1.0,\n",
      "    \"AP@20\": 0.627,\n",
      "    \"F1@20\": 0.667,\n",
      "    \"NDCG@20\": 0.833,\n",
      "    \"MRR\": 1.0,\n",
      "    \"total_relevant\": 10,\n",
      "    \"returned\": 20\n",
      "  },\n",
      "  \"summary\": {\n",
      "    \"K\": 20,\n",
      "    \"MAP\": 0.66,\n",
      "    \"MRR\": 1.0,\n",
      "    \"mean_NDCG@20\": 0.853\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "per_query_metrics = {}\n",
    "ap_list = []\n",
    "rel_lists_for_mrr = []\n",
    "ndcgs = []\n",
    "\n",
    "for qid, ranked_pids in ranked_by_qid.items():\n",
    "    k_use = min(K, len(ranked_pids))\n",
    "    gt_for_q = gt_by_qid[qid]\n",
    "\n",
    "    # build the rel vector for the first K items as provided by the CSV order\n",
    "    rel_ranked = [int(gt_for_q.get(pid, 0)) for pid in ranked_pids[:k_use]]\n",
    "\n",
    "    m = evaluate_query_at_k(ranked_pids, gt_for_q, k=K)\n",
    "\n",
    "    # store rounded\n",
    "    per_query_metrics[qid] = {k: (round(v, 3) if isinstance(v, float) else v) for k, v in m.items()}\n",
    "\n",
    "    ap_list.append(average_precision_at_k(rel_ranked, k_use, sum(gt_for_q.values())))\n",
    "    rel_lists_for_mrr.append(rel_ranked)\n",
    "    ndcgs.append(ndcg_at_k(rel_ranked, k_use))\n",
    "\n",
    "summary = {\n",
    "    \"K\": K,\n",
    "    \"MAP\": mean_average_precision(ap_list),\n",
    "    \"MRR\": mean_reciprocal_rank(rel_lists_for_mrr),\n",
    "    f\"mean_NDCG@{K}\": (sum(ndcgs) / len(ndcgs) if ndcgs else 0.0),\n",
    "}\n",
    "summary_rounded = {k: (round(v, 3) if isinstance(v, float) else v) for k, v in summary.items()}\n",
    "\n",
    "\n",
    "out_struct = {\n",
    "    \"query_1\": per_query_metrics.get(\"1\", {}),\n",
    "    \"query_2\": per_query_metrics.get(\"2\", {}),\n",
    "    \"summary\": summary_rounded,\n",
    "}\n",
    "\n",
    "print(json.dumps(out_struct, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7636a",
   "metadata": {},
   "source": [
    "### **STEP 3 — Expert judges:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b6eed4",
   "metadata": {},
   "source": [
    "#### **Part a)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bc7ea9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling template already exists, not overwritten:\n",
      "  C:\\Users\\Pol\\Documents\\POL\\UNI\\WEB\\irwa-search-engine\\data\\annotations\\queries_label_template.csv\n",
      "If you need to regenerate it, delete the file or set OVERWRITE = True.\n"
     ]
    }
   ],
   "source": [
    "ANN_DIR = DATA_DIR / \"annotations\"\n",
    "ANN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PROPOSED_PATH = DATA_DIR / \"index\" / \"proposed_test_queries.json\"   # from Part 1.2\n",
    "LABEL_TEMPLATE = ANN_DIR / \"queries_label_template.csv\"       \n",
    "\n",
    "# Config\n",
    "K = 10\n",
    "OVERWRITE = False  \n",
    "\n",
    "if not PROPOSED_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Expected your proposed queries at: {PROPOSED_PATH}\")\n",
    "\n",
    "proposed = json.loads(PROPOSED_PATH.read_text(encoding=\"utf-8\"))\n",
    "queries = proposed.get(\"queries\") or proposed.get(\"proposed\") or proposed\n",
    "if not isinstance(queries, list) or not queries:\n",
    "    raise ValueError(\"No queries found in proposed_test_queries.json\")\n",
    "\n",
    "# If the file already exists, don't overwrite (to keep the whole team on the same 50 rows)\n",
    "if LABEL_TEMPLATE.exists() and not OVERWRITE:\n",
    "    print(f\"Labeling template already exists, not overwritten:\\n  {LABEL_TEMPLATE}\")\n",
    "    print(\"If you need to regenerate it, delete the file or set OVERWRITE = True.\")\n",
    "else:\n",
    "    rows = []\n",
    "    qid_counter = 1\n",
    "    for q in queries:\n",
    "        qid = str(qid_counter)\n",
    "        hits = search_tfidf(q, k=K) \n",
    "        for rank, rec in enumerate(hits, start=1):\n",
    "            rows.append({\n",
    "                \"query_id\": qid,\n",
    "                \"query_text\": q,\n",
    "                \"rank\": rank,\n",
    "                \"pid\": rec.get(\"pid\"),\n",
    "                \"title\": (rec.get(\"title\") or \"\")[:120],\n",
    "                \"brand\": rec.get(\"brand\"),\n",
    "                \"category\": rec.get(\"category\"),\n",
    "                \"sub_category\": rec.get(\"sub_category\"),\n",
    "                \"url\": rec.get(\"url\"),\n",
    "                \"label\": \"\"  # we will fill with 1 (relevant) or 0 (not relevant) manually\n",
    "            })\n",
    "        qid_counter += 1\n",
    "\n",
    "    # Write CSV template\n",
    "    with LABEL_TEMPLATE.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a88b19",
   "metadata": {},
   "source": [
    "#### **Part b)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "659d1b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded labeled queries from: C:\\Users\\Pol\\Documents\\POL\\UNI\\WEB\\irwa-search-engine\\data\\annotations\\queries_label_template.csv\n",
      "Found 5 queries. Example qids: ['1', '2', '3', '4', '5']\n"
     ]
    }
   ],
   "source": [
    "import csv, json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Paths\n",
    "LABELED_PATH = DATA_DIR / \"annotations\" / \"queries_label_template.csv\"  # already filled\n",
    "OUT_JSON     = DATA_DIR / \"index\" / \"eval_my_queries.json\"\n",
    "K = 10 \n",
    "\n",
    "if not LABELED_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Cannot find labeled file at: {LABELED_PATH}\")\n",
    "\n",
    "# Build structures\n",
    "ranked_by_qid = defaultdict(list)\n",
    "gt_by_qid = defaultdict(dict)\n",
    "qtext_by_qid = {}\n",
    "\n",
    "with LABELED_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        qid   = str(row[\"query_id\"]).strip()\n",
    "        pid   = str(row[\"pid\"]).strip()\n",
    "        label = str(row.get(\"label\", \"\")).strip()\n",
    "\n",
    "        # store query text (keeps the last seen; all rows per qid share it)\n",
    "        qtext_by_qid[qid] = row.get(\"query_text\", \"\").strip()\n",
    "\n",
    "        # preserve CSV order as ranking\n",
    "        ranked_by_qid[qid].append(pid)\n",
    "\n",
    "        # label must be int 0/1; if empty we will treat as 0\n",
    "        try:\n",
    "            lab = int(label)\n",
    "        except:\n",
    "            lab = 0\n",
    "        gt_by_qid[qid][pid] = 1 if lab == 1 else 0\n",
    "\n",
    "print(f\"Loaded labeled queries from: {LABELED_PATH}\")\n",
    "print(f\"Found {len(ranked_by_qid)} queries. Example qids: {sorted(ranked_by_qid.keys())[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74da5bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'per_query': {'1': {'AP@10': 0.0, 'F1@10': 0.0, 'MRR': 0.0, 'NDCG@10': 0.0, 'P@10': 0.0, 'R@10': 0.0, 'returned': 10, 'total_relevant': 0},\n",
      "               '2': {'AP@10': 0.0, 'F1@10': 0.0, 'MRR': 0.0, 'NDCG@10': 0.0, 'P@10': 0.0, 'R@10': 0.0, 'returned': 10, 'total_relevant': 0},\n",
      "               '3': {'AP@10': 0.43, 'F1@10': 0.571, 'MRR': 0.25, 'NDCG@10': 0.588, 'P@10': 0.4, 'R@10': 1.0, 'returned': 10, 'total_relevant': 4},\n",
      "               '4': {'AP@10': 0.97, 'F1@10': 0.889, 'MRR': 1.0, 'NDCG@10': 0.992, 'P@10': 0.8, 'R@10': 1.0, 'returned': 10, 'total_relevant': 8},\n",
      "               '5': {'AP@10': 0.2, 'F1@10': 0.182, 'MRR': 0.2, 'NDCG@10': 0.387, 'P@10': 0.1, 'R@10': 1.0, 'returned': 10, 'total_relevant': 1}},\n",
      " 'summary': {'K': 10, 'MAP': 0.32, 'MRR': 0.29, 'mean_NDCG@10': 0.393}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pprint\n",
    "from statistics import mean\n",
    "\n",
    "per_query = {}\n",
    "ap_list = []\n",
    "rel_lists = []\n",
    "ndcgs = []\n",
    "\n",
    "for qid in sorted(ranked_by_qid.keys(), key=lambda x: int(x) if x.isdigit() else x):\n",
    "    ranked_pids = ranked_by_qid[qid]\n",
    "    gt_for_q    = gt_by_qid[qid]\n",
    "\n",
    "    # Per-query full metrics at cutoff K\n",
    "    m = evaluate_query_at_k(ranked_pids, gt_for_q, k=K)\n",
    "\n",
    "    # Rounding\n",
    "    per_query[qid] = {k: (round(v, 3) if isinstance(v, float) else v) for k, v in m.items()}\n",
    "\n",
    "    # Build pieces for summary\n",
    "    k_use = min(K, len(ranked_pids))\n",
    "    rel_ranked = [int(gt_for_q.get(pid, 0)) for pid in ranked_pids[:k_use]]\n",
    "    ap_list.append(average_precision_at_k(rel_ranked, k_use, sum(gt_for_q.values())))\n",
    "    rel_lists.append(rel_ranked)\n",
    "    ndcgs.append(ndcg_at_k(rel_ranked, k_use))\n",
    "\n",
    "summary = {\n",
    "    \"K\": K,\n",
    "    \"MAP\": mean_average_precision(ap_list),\n",
    "    \"MRR\": mean_reciprocal_rank(rel_lists),\n",
    "    f\"mean_NDCG@{K}\": (sum(ndcgs) / len(ndcgs) if ndcgs else 0.0),\n",
    "}\n",
    "summary = {k: (round(v, 3) if isinstance(v, float) else v) for k, v in summary.items()}\n",
    "\n",
    "# Save full-precision results\n",
    "OUT_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
    "OUT_JSON.write_text(json.dumps({\"per_query\": per_query, \"summary\": summary}, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "#print(json.dumps({\"per_query\": per_query, \"summary\": summary}, indent=2))\n",
    "\n",
    "pp = pprint.PrettyPrinter(width=200, compact=False)\n",
    "pp.pprint({\"per_query\": per_query, \"summary\": summary})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cc7842",
   "metadata": {},
   "source": [
    "#### **Part c)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "613a376c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5021"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math, csv, json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "K = 10\n",
    "LABELED_CSV = DATA_DIR / \"annotations\" / \"queries_label_template.csv\"\n",
    "OUT_JSON    = DATA_DIR / \"index\" / \"ablation_results.json\"\n",
    "OUT_CSV     = DATA_DIR / \"index\" / \"ablation_results.csv\"\n",
    "\n",
    "if not LABELED_CSV.exists():\n",
    "    raise FileNotFoundError(f\"Missing labeled file: {LABELED_CSV}\")\n",
    "\n",
    "# Loading labeled ranking info (for query texts and ground-truth labels)\n",
    "qid_to_text = {}\n",
    "gt_by_qid   = defaultdict(dict)        # qid -> {pid: 0/1}\n",
    "pids_by_qid = defaultdict(list)    \n",
    "with LABELED_CSV.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    rdr = csv.DictReader(f)\n",
    "    for row in rdr:\n",
    "        qid   = str(row[\"query_id\"]).strip()\n",
    "        pid   = str(row[\"pid\"]).strip()\n",
    "        qtext = (row.get(\"query_text\") or \"\").strip()\n",
    "        lab_s = str(row.get(\"label\") or \"0\").strip()\n",
    "        lab   = 1 if lab_s == \"1\" else 0\n",
    "        qid_to_text[qid] = qtext\n",
    "        gt_by_qid[qid][pid] = lab\n",
    "        pids_by_qid[qid].append(pid)\n",
    "\n",
    "# Helpers to build TF-IDF ranker with field weights\n",
    "from collections import defaultdict as dd\n",
    "\n",
    "def _tokens_from_fields(record, fields):\n",
    "    per_field_counts = {}\n",
    "    for f in fields:\n",
    "        txt = record.get(f)\n",
    "        if not txt:\n",
    "            continue\n",
    "        counts = dd(int)\n",
    "        for t in str(txt).split():\n",
    "            counts[t] += 1\n",
    "        if counts:\n",
    "            per_field_counts[f] = dict(counts)\n",
    "    return per_field_counts\n",
    "\n",
    "def _tf_log2(freq):\n",
    "    return 0.0 if freq <= 0 else 1.0 + math.log(freq, 2)\n",
    "\n",
    "def _idf_log2(df_i, N):\n",
    "    return 0.0 if df_i <= 0 or N <= 0 else math.log(N/df_i, 2)\n",
    "\n",
    "def build_tfidf_ranker(field_weights, fields=(\"title_clean\",\"description_clean\",\"metadata_clean\")):\n",
    "    \"\"\"\n",
    "    Builds a cosine-normalized TF-IDF ranker with per-field weights.\n",
    "    Returns: rank(query, k=K, restrict_doc_ids=None) -> [(doc_id, score), ...]\n",
    "    \"\"\"\n",
    "    N = len(docs)\n",
    "    tf_post    = dd(dict)   # term -> {doc_id: weighted_raw_tf}\n",
    "    df_counts  = dd(int)\n",
    "    doc_norms  = [0.0]*N\n",
    "\n",
    "    # aggregate weighted term counts per doc\n",
    "    for d_id, rec in enumerate(docs):\n",
    "        per_field = _tokens_from_fields(rec, fields)\n",
    "        agg = dd(float)\n",
    "        for fname, counts in per_field.items():\n",
    "            w = field_weights.get(fname, 1.0)\n",
    "            for term, c in counts.items():\n",
    "                agg[term] += w * c\n",
    "        for term, f in agg.items():\n",
    "            tf_post[term][d_id] = f\n",
    "\n",
    "    # df\n",
    "    for term, posting in tf_post.items():\n",
    "        df_counts[term] = len(posting)\n",
    "\n",
    "    # doc norms\n",
    "    for term, posting in tf_post.items():\n",
    "        idf = _idf_log2(df_counts[term], N)\n",
    "        if idf == 0: \n",
    "            continue\n",
    "        for d_id, f in posting.items():\n",
    "            wdt = _tf_log2(f) * idf\n",
    "            if wdt != 0:\n",
    "                doc_norms[d_id] += wdt*wdt\n",
    "    doc_norms = [math.sqrt(x) if x>0 else 0.0 for x in doc_norms]\n",
    "\n",
    "    def rank(query, k=K, restrict_doc_ids=None):\n",
    "        proc = preprocess_text_field(query or \"\")\n",
    "        q_terms = proc[\"tokens\"]\n",
    "        if not q_terms:\n",
    "            return []\n",
    "\n",
    "        # query vector\n",
    "        q_tf = dd(int)\n",
    "        for t in q_terms: q_tf[t]+=1\n",
    "        q_w = {}\n",
    "        q_norm_sq = 0.0\n",
    "        for t, fq in q_tf.items():\n",
    "            df = df_counts.get(t, 0)\n",
    "            if df <= 0:\n",
    "                continue\n",
    "            wt = _tf_log2(fq) * _idf_log2(df, N)\n",
    "            if wt != 0:\n",
    "                q_w[t] = wt\n",
    "                q_norm_sq += wt*wt\n",
    "        q_norm = math.sqrt(q_norm_sq) if q_norm_sq>0 else 0.0\n",
    "        if q_norm == 0.0:\n",
    "            return []\n",
    "\n",
    "        scores = dd(float)\n",
    "        for t, wq in q_w.items():\n",
    "            posting = tf_post.get(t)\n",
    "            if not posting:\n",
    "                continue\n",
    "            idf = _idf_log2(df_counts[t], N)\n",
    "            if idf == 0:\n",
    "                continue\n",
    "            for d_id, f in posting.items():\n",
    "                if restrict_doc_ids is not None and d_id not in restrict_doc_ids:\n",
    "                    continue\n",
    "                wdt = _tf_log2(f) * idf\n",
    "                if wdt != 0:\n",
    "                    scores[d_id] += wq * wdt\n",
    "\n",
    "        ranked = []\n",
    "        for d_id, dot in scores.items():\n",
    "            denom = doc_norms[d_id]*q_norm\n",
    "            if denom>0:\n",
    "                ranked.append((d_id, dot/denom))\n",
    "        ranked.sort(key=lambda x: x[1], reverse=True)\n",
    "        return ranked[:k]\n",
    "    return rank\n",
    "\n",
    "# Configs to test\n",
    "CONFIGS = [\n",
    "    {\"name\":\"tfidf\",         \"field_weights\":{\"title_clean\":2.0, \"description_clean\":1.0, \"metadata_clean\":0.7}, \"and_filter\":False},\n",
    "    {\"name\":\"tfidf_and\",     \"field_weights\":{\"title_clean\":2.0, \"description_clean\":1.0, \"metadata_clean\":0.7}, \"and_filter\":True},\n",
    "    {\"name\":\"tfidf_title3\",  \"field_weights\":{\"title_clean\":3.0, \"description_clean\":1.0, \"metadata_clean\":0.7}, \"and_filter\":False},\n",
    "    {\"name\":\"tfidf_no_meta\", \"field_weights\":{\"title_clean\":2.0, \"description_clean\":1.0, \"metadata_clean\":0.0}, \"and_filter\":False},\n",
    "]\n",
    "\n",
    "# Run ablations\n",
    "from statistics import mean\n",
    "\n",
    "results_table = []\n",
    "per_query_detailed = {}  # config -> {qid -> metrics dict}\n",
    "\n",
    "for cfg in CONFIGS:\n",
    "    ranker = build_tfidf_ranker(cfg[\"field_weights\"])\n",
    "    per_q_metrics = {}\n",
    "    map_list, mrr_list, ndcg_list = [], [], []\n",
    "\n",
    "    for qid in sorted(qid_to_text.keys(), key=lambda x: int(x) if x.isdigit() else x):\n",
    "        qtext = qid_to_text[qid]\n",
    "\n",
    "        restrict_ids = None\n",
    "        if cfg[\"and_filter\"]:\n",
    "            # Boolean AND candidates\n",
    "            and_hits = search_and(qtext, k=50_000)  # uses our boolean index\n",
    "            cand_pids = {h[\"pid\"] for h in and_hits if h.get(\"pid\")}\n",
    "            restrict_ids = {i for i, rec in enumerate(docs) if rec.get(\"pid\") in cand_pids}\n",
    "\n",
    "        ranked_pairs = ranker(qtext, k=K, restrict_doc_ids=restrict_ids)\n",
    "        ranked_pids  = [docs[d_id].get(\"pid\") for d_id, _ in ranked_pairs]\n",
    "        m = evaluate_query_at_k(ranked_pids, gt_by_qid[qid], k=K)\n",
    "        per_q_metrics[qid] = m\n",
    "\n",
    "        map_list.append(m[f\"AP@{K}\"])\n",
    "        mrr_list.append(m[\"MRR\"])\n",
    "        ndcg_list.append(m[f\"NDCG@{K}\"])\n",
    "\n",
    "    per_query_detailed[cfg[\"name\"]] = per_q_metrics\n",
    "    results_table.append({\n",
    "        \"config\": cfg[\"name\"],\n",
    "        \"MAP\": round(mean(map_list), 3) if map_list else 0.0,\n",
    "        \"MRR\": round(mean(mrr_list), 3) if mrr_list else 0.0,\n",
    "        f\"mean_NDCG@{K}\": round(mean(ndcg_list), 3) if ndcg_list else 0.0,\n",
    "    })\n",
    "\n",
    "# Save\n",
    "OUT_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
    "OUT_JSON.write_text(json.dumps({\"summary\": results_table, \"per_query\": per_query_detailed}, ensure_ascii=False, indent=2), encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "269fee0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation summary (higher = better):\n",
      "[\n",
      "  {\n",
      "    \"config\": \"tfidf_and\",\n",
      "    \"MAP\": 0.6,\n",
      "    \"MRR\": 0.6,\n",
      "    \"mean_NDCG@10\": 0.6\n",
      "  },\n",
      "  {\n",
      "    \"config\": \"tfidf_no_meta\",\n",
      "    \"MAP\": 0.483,\n",
      "    \"MRR\": 0.6,\n",
      "    \"mean_NDCG@10\": 0.565\n",
      "  },\n",
      "  {\n",
      "    \"config\": \"tfidf_title3\",\n",
      "    \"MAP\": 0.326,\n",
      "    \"MRR\": 0.29,\n",
      "    \"mean_NDCG@10\": 0.395\n",
      "  },\n",
      "  {\n",
      "    \"config\": \"tfidf\",\n",
      "    \"MAP\": 0.32,\n",
      "    \"MRR\": 0.29,\n",
      "    \"mean_NDCG@10\": 0.393\n",
      "  }\n",
      "]\n",
      "\n",
      "Saved: C:\\Users\\Pol\\Documents\\POL\\UNI\\WEB\\irwa-search-engine\\data\\index\\ablation_results.csv\n"
     ]
    }
   ],
   "source": [
    "#Display results (sorted by MAP) and save a CSV\n",
    "import json, csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the results (the \"just-written\")\n",
    "ablation_path = DATA_DIR / \"index\" / \"ablation_results.json\"\n",
    "res = json.loads(ablation_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "summary = sorted(res[\"summary\"], key=lambda x: x[\"MAP\"], reverse=True)\n",
    "print(\"Ablation summary (higher = better):\")\n",
    "print(json.dumps(summary, indent=2))\n",
    "\n",
    "# Write a compact CSV for the report\n",
    "csv_out = DATA_DIR / \"index\" / \"ablation_results.csv\"\n",
    "with csv_out.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=summary[0].keys())\n",
    "    w.writeheader()\n",
    "    w.writerows(summary)\n",
    "print(f\"\\nSaved: {csv_out}\")\n",
    "\n",
    "# import pprint\n",
    "# pp = pprint.PrettyPrinter(width=160)\n",
    "# pp.pprint(res[\"per_query\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irwa_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
