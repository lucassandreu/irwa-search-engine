{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d943ceea",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# **DELIVERY 2**\n",
    "## **Indexing and Evaluation**\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc77d9b4",
   "metadata": {},
   "source": [
    "## **PART 1: Indexing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b0adad",
   "metadata": {},
   "source": [
    "### **STEP 1 — Build inverted index:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029db64",
   "metadata": {},
   "source": [
    "### **Main Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "182a7618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading enriched dataset: /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/fashion_products_dataset_enriched.json\n",
      "Index will be saved in:   /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/index\n",
      "Loaded 28080 docs\n",
      "Vocabulary size: 9,048\n",
      "Saved inverted index to: /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/index/boolean_inverted_index.json\n",
      "Saved doc map         to: /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/index/docid_pid_map.json\n",
      "Saved fields          to: /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/index/indexed_fields.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set, Any, Iterable\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "REPO_ROOT = NOTEBOOK_DIR.parents[1] if NOTEBOOK_DIR.name in {\"part_1\", \"part_2\"} else NOTEBOOK_DIR\n",
    "sys.path.append(str(REPO_ROOT / \"project_progress\"))\n",
    "from utils.preprocessing import preprocess_text_field\n",
    "\n",
    "\n",
    "\n",
    "# Path\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "REPO_ROOT = NOTEBOOK_DIR.parents[1]          \n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "INPUT = DATA_DIR / \"fashion_products_dataset_enriched.json\"\n",
    "\n",
    "INDEX_DIR = DATA_DIR / \"index\"\n",
    "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INDEX_FILE = INDEX_DIR / \"boolean_inverted_index.json\"\n",
    "DOCMAP_FILE = INDEX_DIR / \"docid_pid_map.json\"\n",
    "FIELDS_FILE = INDEX_DIR / \"indexed_fields.json\"\n",
    "\n",
    "print(f\"Reading enriched dataset: {INPUT}\")\n",
    "print(f\"Index will be saved in:   {INDEX_DIR}\")\n",
    "\n",
    "\n",
    "if not INPUT.exists():\n",
    "    raise FileNotFoundError(f\"Enriched dataset not found: {INPUT}\")\n",
    "docs: List[Dict[str, Any]] = json.loads(INPUT.read_text(encoding=\"utf-8\"))\n",
    "print(f\"Loaded {len(docs)} docs\")\n",
    "\n",
    "\n",
    "INDEXED_TEXT_FIELDS = [\n",
    "    \"title_clean\",\n",
    "    \"description_clean\",\n",
    "    \"metadata_clean\",   \n",
    "]\n",
    "\n",
    "\n",
    "# doc_id is an integer, stable order = index in list\n",
    "docid_to_pid: Dict[int, str] = {}\n",
    "pid_to_docid: Dict[str, int] = {}\n",
    "\n",
    "for i, r in enumerate(docs):\n",
    "    pid = r.get(\"pid\")\n",
    "    if not pid:\n",
    "        pid = r.get(\"_id\", f\"missing_pid_{i}\")\n",
    "    docid_to_pid[i] = pid\n",
    "    pid_to_docid[pid] = i\n",
    "\n",
    "def _doc_tokens(record: Dict[str, Any], fields: Iterable[str]) -> List[str]:\n",
    "    toks: List[str] = []\n",
    "    for f in fields:\n",
    "        val = record.get(f)\n",
    "        if not val:\n",
    "            continue\n",
    "        # We already have cleaned strings; just split.\n",
    "        toks.extend(str(val).split())\n",
    "    return toks\n",
    "\n",
    "\n",
    "# Build inverted index \n",
    "vocab: Dict[str, Set[int]] = defaultdict(set)\n",
    "\n",
    "for doc_id, rec in enumerate(docs):\n",
    "    tokens = _doc_tokens(rec, INDEXED_TEXT_FIELDS)\n",
    "    if not tokens:\n",
    "        continue\n",
    "    # Use unique terms per doc for Boolean presence posting\n",
    "    for term in set(tokens):\n",
    "        vocab[term].add(doc_id)\n",
    "\n",
    "# Convert sets to sorted lists for compactness and efficient AND intersections\n",
    "inverted_index: Dict[str, List[int]] = {t: sorted(list(s)) for t, s in vocab.items()}\n",
    "print(f\"Vocabulary size: {len(inverted_index):,}\")\n",
    "\n",
    "\n",
    "INDEX_FILE.write_text(json.dumps(inverted_index), encoding=\"utf-8\")\n",
    "DOCMAP_FILE.write_text(json.dumps({\"docid_to_pid\": docid_to_pid}, ensure_ascii=False), encoding=\"utf-8\")\n",
    "FIELDS_FILE.write_text(json.dumps({\"indexed_fields\": INDEXED_TEXT_FIELDS}, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Saved inverted index to: {INDEX_FILE}\")\n",
    "print(f\"Saved doc map         to: {DOCMAP_FILE}\")\n",
    "print(f\"Saved fields          to: {FIELDS_FILE}\")\n",
    "\n",
    "\n",
    "REQUIRED_OUTPUT_FIELDS = [\n",
    "    \"pid\", \"title\", \"description\", \"brand\", \"category\", \"sub_category\",\n",
    "    \"product_details\", \"seller\", \"out_of_stock\", \"selling_price\", \"discount\",\n",
    "    \"actual_price\", \"average_rating\", \"url\"\n",
    "]\n",
    "\n",
    "def _query_tokens(q: str) -> List[str]:\n",
    "    # Use the same normalization and stemming pipeline as Step 1\n",
    "    proc = preprocess_text_field(q or \"\")\n",
    "    return proc[\"tokens\"]\n",
    "\n",
    "def _intersect_sorted(a: List[int], b: List[int]) -> List[int]:\n",
    "    \"\"\"Intersect two sorted posting lists.\"\"\"\n",
    "    i=j=0\n",
    "    out: List[int] = []\n",
    "    while i < len(a) and j < len(b):\n",
    "        if a[i] == b[j]:\n",
    "            out.append(a[i])\n",
    "            i+=1; j+=1\n",
    "        elif a[i] < b[j]:\n",
    "            i+=1\n",
    "        else:\n",
    "            j+=1\n",
    "    return out\n",
    "\n",
    "def search_and(query: str, fields: List[str] = None, k: int = 20) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Conjunctive (AND) Boolean search.\n",
    "    Every returned doc must contain ALL query terms (after preprocessing).\n",
    "    Returns up to k full records with the required fields (when present).\n",
    "    \"\"\"\n",
    "    _ = fields  # kept for future extension; current index already built over INDEXED_TEXT_FIELDS\n",
    "    q_terms = _query_tokens(query)\n",
    "    if not q_terms:\n",
    "        return []\n",
    "\n",
    "    # Load postings lists; if any term not in vocab -> empty result\n",
    "    postings_lists: List[List[int]] = []\n",
    "    for t in q_terms:\n",
    "        p = inverted_index.get(t)\n",
    "        if not p:\n",
    "            return []\n",
    "        postings_lists.append(p)\n",
    "\n",
    "    # Intersect from shortest to longest for speed\n",
    "    postings_lists.sort(key=len)\n",
    "    result_ids = postings_lists[0]\n",
    "    for pl in postings_lists[1:]:\n",
    "        result_ids = _intersect_sorted(result_ids, pl)\n",
    "        if not result_ids:\n",
    "            break\n",
    "\n",
    "    # Map to records and keep only required output fields (when present)\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    for did in result_ids[:k]:\n",
    "        rec = docs[did]\n",
    "        # Build a thin view with required fields (include only those present)\n",
    "        view = {f: rec.get(f) for f in REQUIRED_OUTPUT_FIELDS if f in rec}\n",
    "        # Always include pid\n",
    "        if \"pid\" not in view:\n",
    "            view[\"pid\"] = rec.get(\"pid\") or docid_to_pid.get(did)\n",
    "        out.append(view)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c20f01d",
   "metadata": {},
   "source": [
    "### **STEP 2 — Propose test queries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "016d3163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Proposed Test Queries (data-driven) ===\n",
      "1. men shirt solid blue  | tokens=['men', 'shirt', 'solid', 'blue'] | DF-score=50331 | matches≈50\n",
      "2. men cotton shirt blue  | tokens=['men', 'cotton', 'shirt', 'blue'] | DF-score=58230 | matches≈50\n",
      "3. women cotton kurta straight  | tokens=['women', 'cotton', 'kurta', 'straight'] | DF-score=35775 | matches≈50\n",
      "4. men slim fit formal shirt  | tokens=['men', 'slim', 'fit', 'formal', 'shirt'] | DF-score=57466 | matches≈50\n",
      "5. men running shoes black  | tokens=['men', 'run', 'shoe', 'black'] | DF-score=21967 | matches≈28\n",
      "\n",
      "Saved queries to: /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/index/proposed_test_queries.json\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def df(term: str) -> int:\n",
    "    \"\"\"Document frequency of a term (0 if absent).\"\"\"\n",
    "    return len(inverted_index.get(term, []))\n",
    "\n",
    "def in_vocab(token: str) -> bool:\n",
    "    return token in inverted_index\n",
    "\n",
    "def stem_phrase(phrase: str) -> list[str]:\n",
    "    return preprocess_text_field(phrase)[\"tokens\"]\n",
    "\n",
    "def phrase_ok(phrase: str) -> bool:\n",
    "    \"\"\"All tokens exist in vocab AND the AND-query returns at least one result.\"\"\"\n",
    "    toks = stem_phrase(phrase)\n",
    "    if not toks or not all(in_vocab(t) for t in toks):\n",
    "        return False\n",
    "    return len(search_and(phrase, k=1)) > 0\n",
    "\n",
    "def term_popularity_score(tokens: list[str]) -> int:\n",
    "    \"\"\"Sum of dfs for quick 'popularity' proxy.\"\"\"\n",
    "    return sum(df(t) for t in tokens)\n",
    "\n",
    "# Candidate lexicons (raw phrases)\n",
    "gender_phrases  = [\"men\", \"women\"]\n",
    "type_phrases    = [\n",
    "    \"jeans\", \"shirt\", \"t shirt\", \"hoodie\", \"sweatshirt\",\n",
    "    \"track pants\", \"kurta\", \"dress\", \"jacket\"\n",
    "]\n",
    "color_phrases   = [\"black\", \"blue\", \"white\", \"grey\", \"red\", \"green\", \"pink\"]\n",
    "material_phrases= [\"cotton\", \"polyester\", \"denim\", \"linen\", \"silk\"]\n",
    "fit_phrases     = [\"slim\", \"skinny\", \"regular\", \"straight\", \"high waist\"]\n",
    "style_phrases   = [\"printed\", \"solid\", \"striped\", \"floral\"]\n",
    "\n",
    "# Keep only phrases whose tokens exist in vocab (post-stemming)\n",
    "def filter_vocab(phrases):\n",
    "    good = []\n",
    "    for p in phrases:\n",
    "        toks = stem_phrase(p)\n",
    "        if toks and all(in_vocab(t) for t in toks):\n",
    "            good.append((p, toks, term_popularity_score(toks)))\n",
    "    # Sort by popularity descending (PRP)\n",
    "    return sorted(good, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "gender_ok   = filter_vocab(gender_phrases)\n",
    "types_ok    = filter_vocab(type_phrases)\n",
    "colors_ok   = filter_vocab(color_phrases)\n",
    "materials_ok= filter_vocab(material_phrases)\n",
    "fits_ok     = filter_vocab(fit_phrases)\n",
    "styles_ok   = filter_vocab(style_phrases)\n",
    "\n",
    "# Compose candidate query templates: (gender) + (type) + (attribute sets)\n",
    "templates = [\n",
    "    [\"{g}\", \"{m}\", \"{t}\", \"{c}\"],                 # gender + material + type + color\n",
    "    [\"{g}\", \"{t}\", \"{f}\", \"{c}\"],                 # gender + type + fit + color\n",
    "    [\"{g}\", \"full sleeve\", \"{t}\", \"{m}\"],         # gender + sleeve attr + type + material\n",
    "    [\"{g}\", \"{t}\", \"{s}\", \"{c}\"],                 # gender + type + style + color\n",
    "    [\"{g}\", \"high waist\", \"{t}\", \"{c}\"],          # gender + high waist + type + color\n",
    "]\n",
    "\n",
    "def pick(pop_list, k=1):\n",
    "    return [p[0] for p in pop_list[:k]] if pop_list else []\n",
    "\n",
    "# Generate diverse, popular queries that actually return hits\n",
    "proposed = []\n",
    "attempts = 0\n",
    "seen_main_types = set()\n",
    "\n",
    "while len(proposed) < 5 and attempts < 200:\n",
    "    attempts += 1\n",
    "    tpl = random.choice(templates)\n",
    "\n",
    "    g = pick(gender_ok, 1) or [\"women\"]\n",
    "    t = pick(types_ok, 1) or [\"jeans\"]\n",
    "    m = pick(materials_ok, 1) or [\"cotton\"]\n",
    "    c = pick(colors_ok, 1) or [\"blue\"]\n",
    "    f = pick(fits_ok, 1) or [\"slim\"]\n",
    "    s = pick(styles_ok, 1) or [\"printed\"]\n",
    "\n",
    "    phrase = \" \".join(\n",
    "        x.format(g=g[0], t=t[0], m=m[0], c=c[0], f=f[0], s=s[0])\n",
    "        for x in tpl\n",
    "    )\n",
    "    phrase = \" \".join(phrase.split())  # clean double spaces\n",
    "\n",
    "    # Require the AND query to return hits and encourage diversity by not repeating the same main type too much.\n",
    "    main_type = t[0]\n",
    "    if phrase_ok(phrase):\n",
    "        if sum(1 for q in proposed if main_type in q) < 2:\n",
    "            proposed.append(phrase)\n",
    "\n",
    "# Fallbacks (just in case)\n",
    "fallbacks = [\n",
    "    \"women cotton kurta straight\",\n",
    "    \"men slim fit formal shirt\",\n",
    "    \"women high waist blue jeans\",\n",
    "    \"men running shoes black\",\n",
    "    \"women printed dress floral\"\n",
    "]\n",
    "for fb in fallbacks:\n",
    "    if len(proposed) >= 5:\n",
    "        break\n",
    "    if fb not in proposed and phrase_ok(fb):\n",
    "        proposed.append(fb)\n",
    "\n",
    "# Deduplicate and trim to 5\n",
    "seen = set()\n",
    "unique_proposed = []\n",
    "for q in proposed:\n",
    "    if q not in seen:\n",
    "        unique_proposed.append(q)\n",
    "        seen.add(q)\n",
    "proposed = unique_proposed[:5]\n",
    "\n",
    "print(\"=== Proposed Test Queries (data-driven) ===\")\n",
    "for i, q in enumerate(proposed, 1):\n",
    "    hits = len(search_and(q, k=50))\n",
    "    toks = stem_phrase(q)\n",
    "    score = term_popularity_score(toks)\n",
    "    print(f\"{i}. {q}  | tokens={toks} | DF-score={score} | matches≈{hits}\")\n",
    "\n",
    "# Store for later evaluation/report\n",
    "TEST_QUERIES_FILE = DATA_DIR / \"index\" / \"proposed_test_queries.json\"\n",
    "TEST_QUERIES_FILE.write_text(json.dumps({\"queries\": proposed}, indent=2), encoding=\"utf-8\")\n",
    "print(f\"\\nSaved queries to: {TEST_QUERIES_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9279b8",
   "metadata": {},
   "source": [
    "### **STEP 3 — Ranking our results:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66877920",
   "metadata": {},
   "source": [
    "### **Main Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "411254d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "# Configuration\n",
    "INDEXED_TEXT_FIELDS = [\"title_clean\", \"description_clean\", \"metadata_clean\"]\n",
    "FIELD_WEIGHTS: Dict[str, float] = {\n",
    "    \"title_clean\": 2.0,     \n",
    "    \"description_clean\": 1.0,  \n",
    "    \"metadata_clean\": 0.7,     \n",
    "}\n",
    "\n",
    "# Utilities \n",
    "def _tokens_from_fields(record: Dict[str, Any], fields: Iterable[str]) -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Return per-field raw term frequencies:\n",
    "        { field_name : { term : count_in_that_field } }\n",
    "    \"\"\"\n",
    "    per_field_counts: Dict[str, Dict[str, int]] = {}\n",
    "    for f in fields:\n",
    "        txt = record.get(f)\n",
    "        if not txt:\n",
    "            continue\n",
    "        counts: Dict[str, int] = defaultdict(int)\n",
    "        for t in str(txt).split():\n",
    "            counts[t] += 1\n",
    "        if counts:\n",
    "            per_field_counts[f] = dict(counts)\n",
    "    return per_field_counts\n",
    "\n",
    "def _tf_log2(freq: float) -> float:\n",
    "    \"\"\"1 + log2(freq) if freq>0 else 0.\"\"\"\n",
    "    if freq <= 0:\n",
    "        return 0.0\n",
    "    return 1.0 + math.log(freq, 2)\n",
    "\n",
    "def _idf_log2(df_i: int, N: int) -> float:\n",
    "    \"\"\"idf = log2(N / df_i); assumes df_i >= 1.\"\"\"\n",
    "    if df_i <= 0 or N <= 0:\n",
    "        return 0.0\n",
    "    return math.log(N / df_i, 2)\n",
    "\n",
    "# Build TF postings & df\n",
    "N = len(docs)\n",
    "\n",
    "# term -> { doc_id : (field-weighted raw frequency) }\n",
    "tf_postings: Dict[str, Dict[int, float]] = defaultdict(dict)\n",
    "# term -> document frequency\n",
    "df_counts: Dict[str, int] = defaultdict(int)\n",
    "\n",
    "for doc_id, rec in enumerate(docs):\n",
    "    per_field = _tokens_from_fields(rec, INDEXED_TEXT_FIELDS)\n",
    "\n",
    "    term_freq_weighted: Dict[str, float] = defaultdict(float)\n",
    "    for field_name, counts in per_field.items():\n",
    "        w_f = FIELD_WEIGHTS.get(field_name, 1.0)\n",
    "        for term, f_ij_f in counts.items():\n",
    "            term_freq_weighted[term] += w_f * f_ij_f\n",
    "\n",
    "    for term, f_ij in term_freq_weighted.items():\n",
    "        tf_postings[term][doc_id] = f_ij\n",
    "\n",
    "# df_i = number of docs where term appears\n",
    "for term, posting in tf_postings.items():\n",
    "    df_counts[term] = len(posting)\n",
    "\n",
    "# Precompute document norms\n",
    "doc_norms: List[float] = [0.0] * N\n",
    "for term, posting in tf_postings.items():\n",
    "    idf_i = _idf_log2(df_counts[term], N)\n",
    "    if idf_i == 0.0:\n",
    "        continue\n",
    "    for d_id, f_ij in posting.items():\n",
    "        w_dt = _tf_log2(f_ij) * idf_i\n",
    "        if w_dt != 0.0:\n",
    "            doc_norms[d_id] += w_dt * w_dt\n",
    "\n",
    "doc_norms = [math.sqrt(v) if v > 0 else 0.0 for v in doc_norms]\n",
    "\n",
    "# Ranked search\n",
    "def search_tfidf(query: str, k: int = 20) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Rank documents by cosine similarity with TF-IDF (base-2 logs, no smoothing).\n",
    "    Returns top-k with 'score' plus the required output fields.\n",
    "    \"\"\"\n",
    "    q_proc = preprocess_text_field(query or \"\")\n",
    "    q_terms = q_proc[\"tokens\"]\n",
    "    if not q_terms:\n",
    "        return []\n",
    "\n",
    "    # query term frequencies\n",
    "    q_tf: Dict[str, int] = defaultdict(int)\n",
    "    for t in q_terms:\n",
    "        q_tf[t] += 1\n",
    "\n",
    "    # build query vector\n",
    "    q_weights: Dict[str, float] = {}\n",
    "    q_norm_sq = 0.0\n",
    "    for t, f_q in q_tf.items():\n",
    "        df_i = df_counts.get(t, 0)\n",
    "        if df_i <= 0:\n",
    "            continue  # unseen term\n",
    "        w_t = _tf_log2(f_q) * _idf_log2(df_i, N)\n",
    "        if w_t == 0.0:\n",
    "            continue\n",
    "        q_weights[t] = w_t\n",
    "        q_norm_sq += w_t * w_t\n",
    "\n",
    "    q_norm = math.sqrt(q_norm_sq) if q_norm_sq > 0 else 0.0\n",
    "    if q_norm == 0.0:\n",
    "        return []\n",
    "\n",
    "    # sparse dot product over postings of query terms\n",
    "    scores: Dict[int, float] = defaultdict(float)\n",
    "    for t, w_t in q_weights.items():\n",
    "        posting = tf_postings.get(t)\n",
    "        if not posting:\n",
    "            continue\n",
    "        idf_i = _idf_log2(df_counts[t], N)\n",
    "        if idf_i == 0.0:\n",
    "            continue\n",
    "        for d_id, f_ij in posting.items():\n",
    "            w_dt = _tf_log2(f_ij) * idf_i\n",
    "            if w_dt != 0.0:\n",
    "                scores[d_id] += w_t * w_dt\n",
    "\n",
    "    # cosine normalization and rank\n",
    "    ranked: List[Tuple[int, float]] = []\n",
    "    for d_id, dot in scores.items():\n",
    "        denom = doc_norms[d_id] * q_norm\n",
    "        if denom > 0:\n",
    "            ranked.append((d_id, dot / denom))\n",
    "    ranked.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for d_id, sc in ranked[:k]:\n",
    "        rec = docs[d_id]\n",
    "        view = {f: rec.get(f) for f in REQUIRED_OUTPUT_FIELDS if f in rec}\n",
    "        if \"pid\" not in view:\n",
    "            view[\"pid\"] = rec.get(\"pid\")\n",
    "        view[\"score\"] = float(sc)\n",
    "        results.append(view)\n",
    "    return results\n",
    "\n",
    "# AND-filtered TF-IDF\n",
    "def search_tfidf_and(query: str, k: int = 20) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Conjunctive AND filter first (Boolean), then TF-IDF rank within the survivors.\n",
    "    Helpful if your teacher wants AND semantics even for ranking.\n",
    "    \"\"\"\n",
    "    # Candidate set via Boolean AND\n",
    "    cand = search_and(query, k=10_000)\n",
    "    if not cand:\n",
    "        return []\n",
    "\n",
    "    cand_pids = {r[\"pid\"] for r in cand if r.get(\"pid\")}\n",
    "    cand_ids = {i for i, r in enumerate(docs) if r.get(\"pid\") in cand_pids}\n",
    "\n",
    "    # Build query vector (same as search_tfidf)\n",
    "    q_proc = preprocess_text_field(query or \"\")\n",
    "    q_terms = q_proc[\"tokens\"]\n",
    "    if not q_terms:\n",
    "        return []\n",
    "\n",
    "    q_tf: Dict[str, int] = defaultdict(int)\n",
    "    for t in q_terms:\n",
    "        q_tf[t] += 1\n",
    "\n",
    "    q_weights: Dict[str, float] = {}\n",
    "    q_norm_sq = 0.0\n",
    "    for t, f_q in q_tf.items():\n",
    "        df_i = df_counts.get(t, 0)\n",
    "        if df_i <= 0:\n",
    "            continue\n",
    "        w_t = _tf_log2(f_q) * _idf_log2(df_i, N)\n",
    "        if w_t == 0.0:\n",
    "            continue\n",
    "        q_weights[t] = w_t\n",
    "        q_norm_sq += w_t * w_t\n",
    "\n",
    "    q_norm = math.sqrt(q_norm_sq) if q_norm_sq > 0 else 0.0\n",
    "    if q_norm == 0.0:\n",
    "        return []\n",
    "\n",
    "    scores: Dict[int, float] = defaultdict(float)\n",
    "    for t, w_t in q_weights.items():\n",
    "        posting = tf_postings.get(t)\n",
    "        if not posting:\n",
    "            continue\n",
    "        idf_i = _idf_log2(df_counts[t], N)\n",
    "        if idf_i == 0.0:\n",
    "            continue\n",
    "        for d_id, f_ij in posting.items():\n",
    "            if d_id not in cand_ids:\n",
    "                continue\n",
    "            w_dt = _tf_log2(f_ij) * idf_i\n",
    "            if w_dt != 0.0:\n",
    "                scores[d_id] += w_t * w_dt\n",
    "\n",
    "    ranked: List[Tuple[int, float]] = []\n",
    "    for d_id, dot in scores.items():\n",
    "        denom = doc_norms[d_id] * q_norm\n",
    "        if denom > 0:\n",
    "            ranked.append((d_id, dot / denom))\n",
    "    ranked.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for d_id, sc in ranked[:k]:\n",
    "        rec = docs[d_id]\n",
    "        view = {f: rec.get(f) for f in REQUIRED_OUTPUT_FIELDS if f in rec}\n",
    "        if \"pid\" not in view:\n",
    "            view[\"pid\"] = rec.get(\"pid\")\n",
    "        view[\"score\"] = float(sc)\n",
    "        results.append(view)\n",
    "    return results\n",
    "\n",
    "# Persist ranked results\n",
    "def save_ranked_results(out_path: Path, queries: Dict[str, List[str]], use_and_filter: bool = False, k: int = 20) -> Path:\n",
    "    \"\"\"\n",
    "    Save ranked results for groups of queries.\n",
    "    queries = {\"provided\": [q1, q2, ...], \"proposed\": [q3, ...]}\n",
    "    \"\"\"\n",
    "    out = {\"provided_queries\": {}, \"proposed_queries\": {}}\n",
    "    ranker = search_tfidf_and if use_and_filter else search_tfidf\n",
    "\n",
    "    for group, qlist in queries.items():\n",
    "        for q in qlist:\n",
    "            out_key = \"provided_queries\" if group == \"provided\" else \"proposed_queries\"\n",
    "            out[out_key][q] = ranker(q, k=k)\n",
    "\n",
    "    out_path.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5d383",
   "metadata": {},
   "source": [
    "### **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba56b182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF (log2) top for: 'women full sleeve sweatshirt cotton'\n",
      "  0.9074 | SWSFZVTTQCB4SJ7F | Full Sleeve Solid Women Sweatshirt\n",
      "  0.8760 | SWSFQGS456JAZCQB | Full Sleeve Printed Women Sweatshirt\n",
      "  0.8724 | SWSFYTYMNTBNARUN | Full Sleeve Solid Women Sweatshirt\n",
      "\n",
      "TF-IDF (log2) top for: 'men slim jeans blue'\n",
      "  0.7168 | JEAFSKYHZHSZZC9S | Slim Men Blue Jeans\n",
      "  0.7147 | JEAFRAQXEKGUPNUN | Slim Men Blue Jeans\n",
      "  0.7096 | JEAFQF6JBUSEXHVF | Slim Men Blue Jeans\n",
      "\n",
      "Ranked results saved to: /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/index/ranked_results.json\n"
     ]
    }
   ],
   "source": [
    "# Quick demo on course queries\n",
    "for q in [\"women full sleeve sweatshirt cotton\", \"men slim jeans blue\"]:\n",
    "    top = search_tfidf(q, k=3)\n",
    "    print(f\"\\nTF-IDF (log2) top for: {q!r}\")\n",
    "    for r in top:\n",
    "        print(f\"  {r['score']:.4f} | {r.get('pid')} | {(r.get('title') or '')[:80]}\")\n",
    "\n",
    "# Save ranked results for report/repro\n",
    "RANKED_OUT = (DATA_DIR / \"index\" / \"ranked_results.json\")\n",
    "queries_for_report = {\n",
    "    \"provided\": [\n",
    "        \"women full sleeve sweatshirt cotton\",\n",
    "        \"men slim jeans blue\",\n",
    "    ],\n",
    "    # Optionally load your proposed queries file if it exists\n",
    "}\n",
    "pq_file = DATA_DIR / \"index\" / \"proposed_test_queries.json\"\n",
    "if pq_file.exists():\n",
    "    try:\n",
    "        queries_for_report[\"proposed\"] = json.loads(pq_file.read_text(encoding=\"utf-8\"))[\"queries\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "out_path = save_ranked_results(RANKED_OUT, queries_for_report, use_and_filter=False, k=20)\n",
    "print(f\"\\nRanked results saved to: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irwa_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
