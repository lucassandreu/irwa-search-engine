{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "272c526c",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# **DELIVERY 3**\n",
    "## **Ranking & Filtering**\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42077a61",
   "metadata": {},
   "source": [
    "## **PART 1: Three Different Ways of Ranking (TF-IDF + Cosine Similarity, BM25 and Custom)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dad6efd",
   "metadata": {},
   "source": [
    "### **Main Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbff191a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using enriched dataset: /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/fashion_products_dataset_enriched.json\n",
      "Using boolean index:    /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/index/boolean_inverted_index.json\n",
      "Loaded 28080 documents\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import Dict, List, Any, Iterable, Tuple\n",
    "\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "REPO_ROOT = NOTEBOOK_DIR.parents[1] if NOTEBOOK_DIR.name in {\"part_1\", \"part_2\", \"part_3\"} else NOTEBOOK_DIR\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "INDEX_DIR = DATA_DIR / \"index\"\n",
    "\n",
    "sys.path.append(str(REPO_ROOT / \"project_progress\"))\n",
    "from utils.preprocessing import preprocess_text_field  \n",
    "\n",
    "ENRICHED_PATH = DATA_DIR / \"fashion_products_dataset_enriched.json\"\n",
    "INVERTED_PATH = INDEX_DIR / \"boolean_inverted_index.json\"\n",
    "DOCMAP_PATH = INDEX_DIR / \"docid_pid_map.json\"\n",
    "\n",
    "print(f\"Using enriched dataset: {ENRICHED_PATH}\")\n",
    "print(f\"Using boolean index:    {INVERTED_PATH}\")\n",
    "\n",
    "# Load data\n",
    "if not ENRICHED_PATH.exists():\n",
    "    raise FileNotFoundError(ENRICHED_PATH)\n",
    "if not INVERTED_PATH.exists():\n",
    "    raise FileNotFoundError(INVERTED_PATH)\n",
    "\n",
    "docs: List[Dict[str, Any]] = json.loads(ENRICHED_PATH.read_text(encoding=\"utf-8\"))\n",
    "inverted_index: Dict[str, List[int]] = json.loads(INVERTED_PATH.read_text(encoding=\"utf-8\"))\n",
    "docid_to_pid = json.loads(DOCMAP_PATH.read_text(encoding=\"utf-8\"))[\"docid_to_pid\"]\n",
    "\n",
    "N_DOCS = len(docs)\n",
    "print(f\"Loaded {N_DOCS} documents\")\n",
    "\n",
    "# Text fields used for indexing / ranking (same as Part 2)\n",
    "INDEXED_TEXT_FIELDS = [\"title_clean\", \"description_clean\", \"metadata_clean\"]\n",
    "\n",
    "REQUIRED_OUTPUT_FIELDS = [\n",
    "    \"pid\", \"title\", \"description\", \"brand\", \"category\", \"sub_category\",\n",
    "    \"product_details\", \"seller\", \"out_of_stock\", \"selling_price\", \"discount\",\n",
    "    \"actual_price\", \"average_rating\", \"url\"\n",
    "]\n",
    "\n",
    "\n",
    "def _doc_tokens(record: Dict[str, Any], fields: Iterable[str]) -> List[str]:\n",
    "    toks: List[str] = []\n",
    "    for f in fields:\n",
    "        val = record.get(f)\n",
    "        if not val:\n",
    "            continue\n",
    "        toks.extend(str(val).split())\n",
    "    return toks\n",
    "\n",
    "def _query_tokens(q: str) -> List[str]:\n",
    "    proc = preprocess_text_field(q or \"\")\n",
    "    return proc[\"tokens\"]\n",
    "\n",
    "def _intersect_sorted(a: List[int], b: List[int]) -> List[int]:\n",
    "    i = j = 0\n",
    "    out: List[int] = []\n",
    "    while i < len(a) and j < len(b):\n",
    "        if a[i] == b[j]:\n",
    "            out.append(a[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif a[i] < b[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return out\n",
    "\n",
    "def _candidate_docs(q_terms: List[str]) -> List[int]:\n",
    "    \"\"\"\n",
    "    AND semantics over the boolean index:\n",
    "    returns doc_ids that contain ALL query terms.\n",
    "    \"\"\"\n",
    "    if not q_terms:\n",
    "        return []\n",
    "    postings_lists: List[List[int]] = []\n",
    "    for t in set(q_terms):\n",
    "        p = inverted_index.get(t)\n",
    "        if not p:\n",
    "            return []\n",
    "        postings_lists.append(p)\n",
    "    postings_lists.sort(key=len)\n",
    "    result = postings_lists[0]\n",
    "    for pl in postings_lists[1:]:\n",
    "        result = _intersect_sorted(result, pl)\n",
    "        if not result:\n",
    "            break\n",
    "    return result\n",
    "\n",
    "# Pre-compute TF, IDF, norms for TF-IDF and stats for BM25\n",
    "term_df: Dict[str, int] = {t: len(pl) for t, pl in inverted_index.items()}  # df(t)\n",
    "\n",
    "# tf per doc and doc lengths\n",
    "doc_tf: Dict[int, Dict[str, int]] = {}\n",
    "doc_len: Dict[int, int] = {}\n",
    "\n",
    "for doc_id, rec in enumerate(docs):\n",
    "    toks = _doc_tokens(rec, INDEXED_TEXT_FIELDS)\n",
    "    tf = Counter(toks)\n",
    "    doc_tf[doc_id] = dict(tf)\n",
    "    doc_len[doc_id] = sum(tf.values())\n",
    "\n",
    "avg_doc_len = sum(doc_len.values()) / max(N_DOCS, 1)\n",
    "\n",
    "# TF-IDF (log-tf, log2(N/df)) \n",
    "idf_tfidf: Dict[str, float] = {}\n",
    "for t, df in term_df.items():\n",
    "    if df > 0:\n",
    "        idf_tfidf[t] = math.log2(N_DOCS / df)\n",
    "    else:\n",
    "        idf_tfidf[t] = 0.0\n",
    "\n",
    "tfidf_weights: Dict[int, Dict[str, float]] = {}\n",
    "doc_norms: Dict[int, float] = {}\n",
    "\n",
    "for doc_id, tf_map in doc_tf.items():\n",
    "    w_map: Dict[str, float] = {}\n",
    "    sq_sum = 0.0\n",
    "    for t, f in tf_map.items():\n",
    "        if f <= 0:\n",
    "            continue\n",
    "        w = (1.0 + math.log2(f)) * idf_tfidf.get(t, 0.0)\n",
    "        if w != 0.0:\n",
    "            w_map[t] = w\n",
    "            sq_sum += w * w\n",
    "    tfidf_weights[doc_id] = w_map\n",
    "    doc_norms[doc_id] = math.sqrt(sq_sum) if sq_sum > 0 else 0.0\n",
    "\n",
    "# BM25 stats\n",
    "# idf formula: log( (N - df + 0.5) / (df + 0.5) + 1 )\n",
    "idf_bm25: Dict[str, float] = {}\n",
    "for t, df in term_df.items():\n",
    "    num = N_DOCS - df + 0.5\n",
    "    den = df + 0.5\n",
    "    idf_bm25[t] = math.log(num / den + 1.0)\n",
    "\n",
    "k1 = 1.5\n",
    "b = 0.75\n",
    "\n",
    "\n",
    "# Ranking 1: TF-IDF + cosine similarity\n",
    "def _tfidf_cosine_scores(q_terms: List[str], candidate_ids: List[int]) -> Dict[int, float]:\n",
    "    if not candidate_ids:\n",
    "        return {}\n",
    "\n",
    "    # query weights\n",
    "    q_tf = Counter(q_terms)\n",
    "    q_weights: Dict[str, float] = {}\n",
    "    q_sq_sum = 0.0\n",
    "    for t, f in q_tf.items():\n",
    "        if f <= 0:\n",
    "            continue\n",
    "        w = (1.0 + math.log2(f)) * idf_tfidf.get(t, 0.0)\n",
    "        if w != 0.0:\n",
    "            q_weights[t] = w\n",
    "            q_sq_sum += w * w\n",
    "    q_norm = math.sqrt(q_sq_sum) if q_sq_sum > 0 else 0.0\n",
    "    if q_norm == 0.0:\n",
    "        return {}\n",
    "\n",
    "    scores: Dict[int, float] = {}\n",
    "    for did in candidate_ids:\n",
    "        d_weights = tfidf_weights.get(did, {})\n",
    "        denom = doc_norms.get(did, 0.0)\n",
    "        if denom == 0.0:\n",
    "            continue\n",
    "        dot = 0.0\n",
    "        for t, qw in q_weights.items():\n",
    "            dw = d_weights.get(t)\n",
    "            if dw is not None:\n",
    "                dot += qw * dw\n",
    "        if dot > 0.0:\n",
    "            scores[did] = dot / (q_norm * denom)\n",
    "    return scores\n",
    "\n",
    "def search_tfidf_cosine(query: str, k: int = 20) -> List[Dict[str, Any]]:\n",
    "    q_terms = _query_tokens(query)\n",
    "    cand_ids = _candidate_docs(q_terms)\n",
    "    scores = _tfidf_cosine_scores(q_terms, cand_ids)\n",
    "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for did, s in ranked:\n",
    "        rec = docs[did]\n",
    "        view = {f: rec.get(f) for f in REQUIRED_OUTPUT_FIELDS if f in rec}\n",
    "        view[\"pid\"] = rec.get(\"pid\") or docid_to_pid.get(str(did))\n",
    "        view[\"score_tfidf\"] = s\n",
    "        results.append(view)\n",
    "    return results\n",
    "\n",
    "\n",
    "# Ranking 2: BM25\n",
    "def _bm25_scores(q_terms: List[str], candidate_ids: List[int]) -> Dict[int, float]:\n",
    "    if not candidate_ids:\n",
    "        return {}\n",
    "    q_unique = list(set(q_terms))  # BM25 usually ignores query term frequency or uses min(1, tf)\n",
    "\n",
    "    scores: Dict[int, float] = {}\n",
    "    for did in candidate_ids:\n",
    "        tf_map = doc_tf.get(did, {})\n",
    "        dl = doc_len.get(did, 0)\n",
    "        if dl == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        for t in q_unique:\n",
    "            f = tf_map.get(t, 0)\n",
    "            if f <= 0:\n",
    "                continue\n",
    "            idf = idf_bm25.get(t, 0.0)\n",
    "            denom = f + k1 * (1.0 - b + b * dl / avg_doc_len)\n",
    "            score += idf * (f * (k1 + 1.0) / denom)\n",
    "        if score != 0.0:\n",
    "            scores[did] = score\n",
    "    return scores\n",
    "\n",
    "def search_bm25(query: str, k: int = 20) -> List[Dict[str, Any]]:\n",
    "    q_terms = _query_tokens(query)\n",
    "    cand_ids = _candidate_docs(q_terms)\n",
    "    scores = _bm25_scores(q_terms, cand_ids)\n",
    "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for did, s in ranked:\n",
    "        rec = docs[did]\n",
    "        view = {f: rec.get(f) for f in REQUIRED_OUTPUT_FIELDS if f in rec}\n",
    "        view[\"pid\"] = rec.get(\"pid\") or docid_to_pid.get(str(did))\n",
    "        view[\"score_bm25\"] = s\n",
    "        results.append(view)\n",
    "    return results\n",
    "\n",
    "\n",
    "# Ranking 3: Custom score (TF-IDF + numeric boosts)\n",
    "def _numeric_boost(rec: Dict[str, Any]) -> float:\n",
    "    \"\"\"\n",
    "    Combines rating, discount, price and stock availability into a single multiplier.\n",
    "    Idea:\n",
    "      - Prefer in-stock items\n",
    "      - Higher rating and higher discount -> better\n",
    "      - Slight preference for cheaper items (within a cap)\n",
    "    \"\"\"\n",
    "    rating = rec.get(\"average_rating_num\") or 0.0\n",
    "    discount = rec.get(\"discount_pct\") or 0\n",
    "    price = rec.get(\"selling_price_num\") or rec.get(\"actual_price_num\") or 0.0\n",
    "    out_of_stock = rec.get(\"out_of_stock_bool\")\n",
    "\n",
    "    # Normalize\n",
    "    rating_norm = max(0.0, min(rating / 5.0, 1.0))          # 0â€“1\n",
    "    discount_norm = max(0.0, min(discount / 80.0, 1.0))     # assume 80% is \"very high\"\n",
    "    price_cap = 4000.0\n",
    "    if price <= 0:\n",
    "        price_norm = 0.5\n",
    "    else:\n",
    "        price_norm = 1.0 - min(price, price_cap) / price_cap  # cheaper -> closer to 1\n",
    "\n",
    "    stock_factor = 1.0 if not out_of_stock else 0.2         # strong penalty if out of stock\n",
    "\n",
    "    # Weighted combination -> multiplier around ~[0.5, 2]\n",
    "    boost = 1.0 + 0.5 * rating_norm + 0.4 * discount_norm + 0.3 * price_norm\n",
    "    return boost * stock_factor\n",
    "\n",
    "def search_custom_score(query: str, k: int = 20) -> List[Dict[str, Any]]:\n",
    "    q_terms = _query_tokens(query)\n",
    "    cand_ids = _candidate_docs(q_terms)\n",
    "\n",
    "    # base relevance: TF-IDF cosine\n",
    "    base_scores = _tfidf_cosine_scores(q_terms, cand_ids)\n",
    "    if not base_scores:\n",
    "        return []\n",
    "\n",
    "    final_scores: Dict[int, float] = {}\n",
    "    for did, base in base_scores.items():\n",
    "        rec = docs[did]\n",
    "        boost = _numeric_boost(rec)\n",
    "        final_scores[did] = base * boost\n",
    "\n",
    "    ranked = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for did, s in ranked:\n",
    "        rec = docs[did]\n",
    "        view = {f: rec.get(f) for f in REQUIRED_OUTPUT_FIELDS if f in rec}\n",
    "        view[\"pid\"] = rec.get(\"pid\") or docid_to_pid.get(str(did))\n",
    "        view[\"score_custom\"] = s\n",
    "        results.append(view)\n",
    "    return results\n",
    "\n",
    "# Small helper to try all three methods with the same query\n",
    "def compare_rankers(query: str, k: int = 5):\n",
    "    print(f\"\\n=== Query: {query!r} ===\")\n",
    "\n",
    "    tfidf_res = search_tfidf_cosine(query, k=k)\n",
    "    bm25_res = search_bm25(query, k=k)\n",
    "    custom_res = search_custom_score(query, k=k)\n",
    "\n",
    "    def _show(label: str, res: List[Dict[str, Any]], score_key: str):\n",
    "        print(f\"\\n-- {label} --\")\n",
    "        for r in res:\n",
    "            title = (r.get(\"title\") or \"\")[:60]\n",
    "            pid = r.get(\"pid\")\n",
    "            s = r.get(score_key)\n",
    "            print(f\"{pid} | {s:.4f} | {title}\")\n",
    "\n",
    "    _show(\"TF-IDF + cosine\", tfidf_res, \"score_tfidf\")\n",
    "    _show(\"BM25\", bm25_res, \"score_bm25\")\n",
    "    _show(\"Custom score\", custom_res, \"score_custom\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8356618d",
   "metadata": {},
   "source": [
    "### **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6c5fd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Query: 'women full sleeve sweatshirt cotton' ===\n",
      "\n",
      "-- TF-IDF + cosine --\n",
      "SWSFZVTTQCB4SJ7F | 0.7305 | Full Sleeve Solid Women Sweatshirt\n",
      "SWSFQGS456JAZCQB | 0.6561 | Full Sleeve Printed Women Sweatshirt\n",
      "SWSFZVTNGM4HG8BC | 0.6339 | Full Sleeve Printed Women Sweatshirt\n",
      "SWSFYTYMNTBNARUN | 0.6237 | Full Sleeve Solid Women Sweatshirt\n",
      "SWSFYRKYAHH4HHSM | 0.6070 | Full Sleeve Printed Women Sweatshirt\n",
      "\n",
      "-- BM25 --\n",
      "SWSFVZRFS7GHGKSF | 10.2591 | Full Sleeve Solid Women Sweatshirt\n",
      "SWSFYFFFFYZ896TJ | 9.9860 | Full Sleeve Printed Women Sweatshirt\n",
      "SWSFXMFPDVRHYYPH | 9.9796 | Full Sleeve Striped Women Sweatshirt\n",
      "SWSFXMFPPZGDQGMW | 9.9796 | Full Sleeve Striped Women Sweatshirt\n",
      "SWSFYFFYQ7Z3ZKN6 | 9.9541 | Full Sleeve Printed Women Sweatshirt\n",
      "\n",
      "-- Custom score --\n",
      "SWSFZVTNGM4HG8BC | 1.2278 | Full Sleeve Printed Women Sweatshirt\n",
      "SWSFYTYMNTBNARUN | 1.2057 | Full Sleeve Solid Women Sweatshirt\n",
      "SWSFY38ADYPVZHYZ | 1.1757 | Full Sleeve Printed Women Sweatshirt\n",
      "SWSFY382UZHFBCNB | 1.1277 | Full Sleeve Printed Women Sweatshirt\n",
      "SWSFZ2H4KMYXZXX7 | 1.1058 | Full Sleeve Solid Women Sweatshirt\n",
      "\n",
      "=== Query: 'men slim jeans blue' ===\n",
      "\n",
      "-- TF-IDF + cosine --\n",
      "JEAFRAQXEKGUPNUN | 0.4843 | Slim Men Blue Jeans\n",
      "JEAFSKYHZHSZZC9S | 0.4559 | Slim Men Blue Jeans\n",
      "JEAFQF6JBUSEXHVF | 0.4447 | Slim Men Blue Jeans\n",
      "JEAFSKYHRVZSABPR | 0.4423 | Slim Men Blue Jeans\n",
      "JEAFSKYHTE76YWH7 | 0.4423 | Slim Men Blue Jeans\n",
      "\n",
      "-- BM25 --\n",
      "JEAEVJGUSXRNSHRY | 10.2389 | Slim Men Dark Blue Jeans\n",
      "JEAFSKYHRVZSABPR | 10.1051 | Slim Men Blue Jeans\n",
      "JEAFSKYHTE76YWH7 | 10.1051 | Slim Men Blue Jeans\n",
      "JEAEHGRJSGGYEYYX | 10.0824 | Slim Men Light Blue Jeans\n",
      "JEAFWZXTFGMP9GCN | 9.9718 | Slim Men Blue Jeans\n",
      "\n",
      "-- Custom score --\n",
      "JEAFQF6JBUSEXHVF | 0.7882 | Slim Men Blue Jeans\n",
      "JEAFSKYHZHSZZC9S | 0.7670 | Slim Men Blue Jeans\n",
      "JEAFSKYHRVZSABPR | 0.7660 | Slim Men Blue Jeans\n",
      "JEAFESND4QWQUBZD | 0.7660 | Slim Men Blue Jeans\n",
      "JEAFEKQZ4C2Z6GCX | 0.7643 | Slim Men Blue Jeans\n"
     ]
    }
   ],
   "source": [
    "compare_rankers(\"women full sleeve sweatshirt cotton\", k=5)\n",
    "compare_rankers(\"men slim jeans blue\", k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca14531",
   "metadata": {},
   "source": [
    "## **PART 2: Word2vec + Cosine Ranking Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33601174",
   "metadata": {},
   "source": [
    "### **Main Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d41d541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Iterable, Tuple\n",
    "import json, math\n",
    "import numpy as np\n",
    "\n",
    "# gensim allowed for Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "W2V_MODEL_PATH = INDEX_DIR / \"word2vec.model\"\n",
    "DOCVECS_PATH   = INDEX_DIR / \"word2vec_docvecs.npy\"   # dense matrix (n_docs, dim)\n",
    "DOCMASK_PATH   = INDEX_DIR / \"word2vec_docmask.npy\"   # boolean mask: doc has at least 1 in-vocab token\n",
    "META_PATH      = INDEX_DIR / \"word2vec_meta.json\"     # stores dim, min_count, etc.\n",
    "\n",
    "# Helpers: training corpus\n",
    "def _sentences_from_docs(records: List[Dict[str, Any]], fields: Iterable[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Build training sentences from cleaned fields.\n",
    "    Each doc contributes one sentence per field (tokenized by .split()).\n",
    "    \"\"\"\n",
    "    sents: List[List[str]] = []\n",
    "    for r in records:\n",
    "        for f in fields:\n",
    "            txt = r.get(f)\n",
    "            if not txt:\n",
    "                continue\n",
    "            toks = str(txt).split()\n",
    "            if toks:\n",
    "                sents.append(toks)\n",
    "    return sents\n",
    "\n",
    "# Train (or load) Word2Vec\n",
    "def get_or_train_w2v(\n",
    "    records: List[Dict[str, Any]],\n",
    "    fields: Iterable[str],\n",
    "    vector_size: int = 100,\n",
    "    window: int = 5,\n",
    "    min_count: int = 2,\n",
    "    workers: int = 4,\n",
    "    sg: int = 1,  # skip-gram (1) tends to work better for semantic similarity\n",
    ") -> Word2Vec:\n",
    "    if W2V_MODEL_PATH.exists():\n",
    "        model = Word2Vec.load(str(W2V_MODEL_PATH))\n",
    "        return model\n",
    "\n",
    "    sents = _sentences_from_docs(records, fields)\n",
    "    model = Word2Vec(\n",
    "        sentences=sents,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "        sg=sg,\n",
    "        epochs=10,\n",
    "    )\n",
    "    model.save(str(W2V_MODEL_PATH))\n",
    "    META_PATH.write_text(json.dumps({\n",
    "        \"vector_size\": vector_size,\n",
    "        \"window\": window,\n",
    "        \"min_count\": min_count,\n",
    "        \"sg\": sg,\n",
    "        \"epochs\": 10\n",
    "    }, indent=2))\n",
    "    return model\n",
    "\n",
    "# Building / loading document vectors\n",
    "def _avg_vec(tokens: List[str], model: Word2Vec) -> np.ndarray:\n",
    "    \"\"\"Average word vectors for tokens in model; return zero vector if none in vocab.\"\"\"\n",
    "    vecs = []\n",
    "    for t in tokens:\n",
    "        if t in model.wv:\n",
    "            vecs.append(model.wv[t])\n",
    "    if not vecs:\n",
    "        return np.zeros(model.vector_size, dtype=np.float32)\n",
    "    arr = np.vstack(vecs).mean(axis=0)\n",
    "    return arr.astype(np.float32)\n",
    "\n",
    "def build_or_load_doc_matrix(model: Word2Vec) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      doc_mat: (N_DOCS, dim) float32\n",
    "      has_vec: (N_DOCS,) boolean, True if doc has at least 1 in-vocab token\n",
    "    \"\"\"\n",
    "    if DOCVECS_PATH.exists() and DOCMASK_PATH.exists():\n",
    "        doc_mat = np.load(DOCVECS_PATH)\n",
    "        has_vec = np.load(DOCMASK_PATH)\n",
    "        return doc_mat, has_vec\n",
    "\n",
    "    dim = model.vector_size\n",
    "    doc_mat = np.zeros((N_DOCS, dim), dtype=np.float32)\n",
    "    has_vec = np.zeros(N_DOCS, dtype=bool)\n",
    "\n",
    "    for did, r in enumerate(docs):\n",
    "        toks: List[str] = []\n",
    "        for f in INDEXED_TEXT_FIELDS:\n",
    "            txt = r.get(f)\n",
    "            if txt:\n",
    "                toks.extend(str(txt).split())\n",
    "        v = _avg_vec(toks, model)\n",
    "        doc_mat[did, :] = v\n",
    "        has_vec[did] = np.any(v != 0.0)\n",
    "\n",
    "    np.save(DOCVECS_PATH, doc_mat)\n",
    "    np.save(DOCMASK_PATH, has_vec)\n",
    "    return doc_mat, has_vec\n",
    "\n",
    "# Cosine similarity\n",
    "def _cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    da = float(np.linalg.norm(a))\n",
    "    db = float(np.linalg.norm(b))\n",
    "    if da == 0.0 or db == 0.0:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / (da * db))\n",
    "\n",
    "# Word2Vec search (AND-filtered)\n",
    "def search_w2v_cosine(query: str, k: int = 20) -> List[Dict[str, Any]]:\n",
    "    # Ensuring model + vectors exist\n",
    "    model = get_or_train_w2v(docs, INDEXED_TEXT_FIELDS)\n",
    "    doc_mat, has_vec = build_or_load_doc_matrix(model)\n",
    "\n",
    "    # Query processing -> tokens (reuse the same preprocessing used everywhere)\n",
    "    q_tokens = preprocess_text_field(query or \"\")[\"tokens\"]\n",
    "    q_vec = _avg_vec(q_tokens, model)\n",
    "    if not np.any(q_vec):\n",
    "        return []  # nothing in-vocab, no good semantic signal\n",
    "\n",
    "    # AND candidate set\n",
    "    q_terms_for_bool = _query_tokens(query)\n",
    "    cand_ids = _candidate_docs(q_terms_for_bool)\n",
    "    if not cand_ids:\n",
    "        return []\n",
    "\n",
    "    # Score candidates by cosine\n",
    "    scores: Dict[int, float] = {}\n",
    "    for did in cand_ids:\n",
    "        if not has_vec[did]:\n",
    "            continue\n",
    "        dv = doc_mat[did, :]\n",
    "        s = _cosine(q_vec, dv)\n",
    "        if s > 0.0:\n",
    "            scores[did] = s\n",
    "\n",
    "    if not scores:\n",
    "        return []\n",
    "\n",
    "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for did, s in ranked:\n",
    "        rec = docs[did]\n",
    "        view = {f: rec.get(f) for f in REQUIRED_OUTPUT_FIELDS if f in rec}\n",
    "        view[\"pid\"] = rec.get(\"pid\") or docid_to_pid.get(str(did))\n",
    "        view[\"score_w2v\"] = float(s)\n",
    "        results.append(view)\n",
    "    return results\n",
    "\n",
    "# Batch run for our queries from Part 2\n",
    "def run_word2vec_on_proposed(k: int = 20, save_path: Path = (INDEX_DIR / \"w2v_results.json\")) -> Path:\n",
    "    \"\"\"\n",
    "    Loads the five proposed queries created in Part 2 (proposed_test_queries.json)\n",
    "    and saves top-k results per query ranked with Word2Vec + cosine.\n",
    "    \"\"\"\n",
    "    qfile = INDEX_DIR / \"proposed_test_queries.json\"\n",
    "    if not qfile.exists():\n",
    "        print(f\"[INFO] Proposed queries file not found at {qfile}; run your Part 2 query-mining first.\")\n",
    "        return save_path\n",
    "\n",
    "    queries = json.loads(qfile.read_text(encoding=\"utf-8\")).get(\"queries\", [])\n",
    "    out = {}\n",
    "    for q in queries:\n",
    "        hits = search_w2v_cosine(q, k=k)\n",
    "        out[q] = [{\"pid\": h.get(\"pid\"), \"title\": h.get(\"title\"), \"score_w2v\": h.get(\"score_w2v\")} for h in hits]\n",
    "\n",
    "    save_path.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"Wrote Word2Vec top-{k} for {len(queries)} queries to: {save_path}\")\n",
    "    return save_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58063b8",
   "metadata": {},
   "source": [
    "### **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55ee9382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== women full sleeve sweatshirt cotton ==\n",
      "SWSFY5ZHUEZPZZYV | 0.8426 | Full Sleeve Printed Women Sweatshirt\n",
      "SWSF9W528G7VEGCV | 0.8423 | Full Sleeve Striped Women Sweatshirt\n",
      "SWSFY5ZHEJ2HYWDG | 0.8421 | Full Sleeve Printed Men Sweatshirt\n",
      "SWSF9W5YHFAAHNJZ | 0.8404 | Full Sleeve Striped Women Sweatshirt\n",
      "SWSFMJF98EY2FXBH | 0.8377 | Full Sleeve Solid Women Sweatshirt\n",
      "\n",
      "== men slim jeans blue ==\n",
      "JEAFSKYHRVZSABPR | 0.7829 | Slim Men Blue Jeans\n",
      "JEAFSKYHTE76YWH7 | 0.7829 | Slim Men Blue Jeans\n",
      "JEAFSKYH539HTZB8 | 0.7803 | Tapered Fit Men Blue Jeans\n",
      "JEAFXUHCWV9C5WNX | 0.7783 | Super Skinny Men Blue Jeans\n",
      "JEAFXUHA5YF8WYQY | 0.7780 | Tapered Fit Men Blue Jeans\n",
      "Wrote Word2Vec top-20 for 4 queries to: /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/index/w2v_results.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/index/w2v_results.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single query demo\n",
    "for q in [\"women full sleeve sweatshirt cotton\", \"men slim jeans blue\"]:\n",
    "    res = search_w2v_cosine(q, k=5)\n",
    "    print(f\"\\n== {q} ==\")\n",
    "    for r in res:\n",
    "        print(f\"{r['pid']} | {r['score_w2v']:.4f} | {(r.get('title') or '')[:70]}\")\n",
    "\n",
    "# Batch on our queries saved in Part 2\n",
    "run_word2vec_on_proposed(k=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irwa_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
