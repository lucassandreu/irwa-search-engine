{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da66747f",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# **DELIVERY 1**\n",
    "## **TEXT PROCESSING & EXPLORATORY DATA ANALYSIS (EDA)**\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bec7e2",
   "metadata": {},
   "source": [
    "## **PART 1: DATA PREPARATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bac50d9",
   "metadata": {},
   "source": [
    "### **STEP 1 — Preprocessing Pipeline (NLTK)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c4f09",
   "metadata": {},
   "source": [
    "### **Main Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c8b95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset: /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/fashion_products_dataset.json\n",
      "Output will be saved to: /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/fashion_products_dataset_preprocessed.json\n",
      "Processed 28080 records.\n",
      "Saved to /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/fashion_products_dataset_preprocessed.json\n",
      "Example output:\n",
      "\n",
      "{\n",
      "  \"_id\": \"fa8e22d6-c0b6-5229-bb9e-ad52eda39a0a\",\n",
      "  \"actual_price\": \"2,999\",\n",
      "  \"average_rating\": \"3.9\",\n",
      "  \"brand\": \"York\",\n",
      "  \"category\": \"Clothing and Accessories\",\n",
      "  \"crawled_at\": 1612987911000,\n",
      "  \"description\": \"Yorker trackpants made from 100% rich combed cotton giving it a rich look.Designed for Comfort,Skin friendly fabric,itch-free waistband & great for all year round use Proudly made in India\",\n",
      "  \"discount\": \"69% off\",\n",
      "  \"images\": [\n",
      "    \"https://rukminim1.flixcart.com/image/128/128/jr3t5e80/track-pant/z/y/n/m-1005combo2-yorker-original-imafczg3xfh5qqd4.jpeg?q=70\",\n",
      "    \"https://rukminim1.flixcart.com/image/128/128/jr58l8w0/track-pant/w/d/a/l-1005combo8-yorker-original-imafczg3pgtxgraq.jpeg?q=70\"\n",
      "  ],\n",
      "  \"out_of_stock\": false,\n",
      "  \"pid\": \"TKPFCZ9EA7H5FYZH\",\n",
      "  \"product_details\": [\n",
      "    {\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from unidecode import unidecode\n",
    "\n",
    "NOTEBOOK_DIR = Path(__file__).resolve().parent if \"__file__\" in globals() else Path().resolve()\n",
    "REPO_ROOT = NOTEBOOK_DIR.parents[1] if NOTEBOOK_DIR.name == \"part_1\" else NOTEBOOK_DIR\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "\n",
    "INPUT_FILE = DATA_DIR / \"fashion_products_dataset.json\"\n",
    "OUTPUT_FILE = DATA_DIR / \"fashion_products_dataset_preprocessed.json\"\n",
    "\n",
    "print(f\"Using dataset: {INPUT_FILE}\")\n",
    "print(f\"Output will be saved to: {OUTPUT_FILE}\")\n",
    "\n",
    "def _ensure_nltk():\n",
    "    try:\n",
    "        stopwords.words(\"english\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"stopwords\")\n",
    "    try:\n",
    "        word_tokenize(\"test\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"punkt\")\n",
    "_ensure_nltk()\n",
    "\n",
    "\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "\n",
    "def normalize_basic(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Basic normalization:\n",
    "      - Lowercase\n",
    "      - Remove accents with unidecode\n",
    "      - Remove currency symbols and numbers\n",
    "      - Remove punctuation marks\n",
    "      - Replace dashes/underscores with space\n",
    "      - Collapse extra whitespace\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    txt = unidecode(text)\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(r\"http[s]?://\\S+\", \" \", txt)\n",
    "    txt = re.sub(r\"[-_/]\", \" \", txt)\n",
    "    txt = re.sub(r\"[$€£₹¥%]+\", \" \", txt)\n",
    "    txt = re.sub(r\"\\d+\", \" \", txt)\n",
    "    txt = re.sub(r\"[^\\w\\s]\", \" \", txt)\n",
    "    txt = re.sub(r\"[^a-z\\s]\", \" \", txt)\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "    return txt\n",
    "\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return word_tokenize(text) if text else []\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens: List[str]) -> List[str]:\n",
    "    return [t for t in tokens if t not in STOPWORDS]\n",
    "\n",
    "\n",
    "def stem(tokens: List[str]) -> List[str]:\n",
    "    return [STEMMER.stem(t) for t in tokens]\n",
    "\n",
    "\n",
    "def preprocess_text_field(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Apply the full preprocessing pipeline to one text field.\"\"\"\n",
    "    cleaned = normalize_basic(text)\n",
    "    toks = tokenize(cleaned)\n",
    "    toks_nostop = remove_stopwords(toks)\n",
    "    toks_stem = stem(toks_nostop)\n",
    "    return {\n",
    "        \"tokens\": toks_stem,\n",
    "        \"text\": \" \".join(toks_stem)\n",
    "    }\n",
    "\n",
    "\n",
    "def process_record(rec: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Process one record and add preprocessed fields.\"\"\"\n",
    "    title = rec.get(\"title\", \"\")\n",
    "    desc = rec.get(\"description\", \"\")\n",
    "\n",
    "    t_proc = preprocess_text_field(title)\n",
    "    d_proc = preprocess_text_field(desc)\n",
    "\n",
    "    rec_out = dict(rec)\n",
    "    rec_out[\"title_tokens\"] = t_proc[\"tokens\"]\n",
    "    rec_out[\"title_clean\"] = t_proc[\"text\"]\n",
    "    rec_out[\"description_tokens\"] = d_proc[\"tokens\"]\n",
    "    rec_out[\"description_clean\"] = d_proc[\"text\"]\n",
    "    return rec_out\n",
    "\n",
    "\n",
    "def read_json(path: Path):\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        if not isinstance(data, list):\n",
    "            raise ValueError(\"Expected a JSON array.\")\n",
    "        return data\n",
    "\n",
    "\n",
    "def write_json(path: Path, items):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(items, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "if not INPUT_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Input file not found: {INPUT_FILE}\")\n",
    "\n",
    "data = read_json(INPUT_FILE)\n",
    "processed = [process_record(rec) for rec in data]\n",
    "write_json(OUTPUT_FILE, processed)\n",
    "\n",
    "print(f\"Processed {len(processed)} records.\")\n",
    "print(f\"Saved to {OUTPUT_FILE}\")\n",
    "print(\"Example output:\\n\")\n",
    "print(json.dumps(processed[0], indent=2)[:800])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559f932e",
   "metadata": {},
   "source": [
    "### **Verification Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24520638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing fields in first 200 records: [] (showing up to 10)\n",
      "\n",
      "TITLE RAW:  Solid Women Multicolor Track Pants\n",
      "TITLE CLEAN: solid women multicolor track pant\n",
      "TITLE TOKENS: ['solid', 'women', 'multicolor', 'track', 'pant']\n",
      "DESC CLEAN: yorker trackpant made rich comb cotton give rich look design comfort skin friendli fabric itch free waistband great year ...\n",
      "\n",
      "TITLE RAW:  Solid Men Blue Track Pants\n",
      "TITLE CLEAN: solid men blue track pant\n",
      "TITLE TOKENS: ['solid', 'men', 'blue', 'track', 'pant']\n",
      "DESC CLEAN: yorker trackpant made rich comb cotton give rich look design comfort skin friendli fabric itch free waistband great year ...\n",
      "\n",
      "TITLE RAW:  Solid Men Multicolor Track Pants\n",
      "TITLE CLEAN: solid men multicolor track pant\n",
      "TITLE TOKENS: ['solid', 'men', 'multicolor', 'track', 'pant']\n",
      "DESC CLEAN: yorker trackpant made rich comb cotton give rich look design comfort skin friendli fabric itch free waistband great year ...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, itertools\n",
    "\n",
    "out_path = Path(\"../..\") / \"data\" / \"fashion_products_dataset_preprocessed.json\"  # adjust if needed\n",
    "data = json.loads(Path(out_path).read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# 1) Confirm new keys exist\n",
    "required_keys = {\"title_tokens\",\"title_clean\",\"description_tokens\",\"description_clean\"}\n",
    "missing = [i for i,r in enumerate(data[:200]) if not required_keys.issubset(r)]\n",
    "print(\"Missing fields in first 200 records:\", missing[:10], \"(showing up to 10)\")\n",
    "\n",
    "# 2) Spot-check a few examples (before vs after)\n",
    "for rec in itertools.islice((r for r in data if r.get(\"title\")), 3):\n",
    "    print(\"\\nTITLE RAW: \", rec[\"title\"])\n",
    "    print(\"TITLE CLEAN:\", rec[\"title_clean\"])\n",
    "    print(\"TITLE TOKENS:\", rec[\"title_tokens\"][:15])\n",
    "    print(\"DESC CLEAN:\", rec[\"description_clean\"][:120], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68afb3cb",
   "metadata": {},
   "source": [
    "### **STEP 2 — Non-Text Fields Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac29f35c",
   "metadata": {},
   "source": [
    "### **Main Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "240f0172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using preprocessed dataset: /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/fashion_products_dataset_preprocessed.json\n",
      "Enriched output will be saved to: /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/fashion_products_dataset_enriched.json\n",
      "Enriched 28080 records.\n",
      "Saved to /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/fashion_products_dataset_enriched.json\n",
      "Example (keys excerpt): ['_id', 'actual_price', 'average_rating', 'brand', 'category', 'crawled_at', 'description', 'discount', 'images', 'out_of_stock', 'pid', 'product_details', 'seller', 'selling_price', 'sub_category', 'title', 'url', 'title_tokens', 'title_clean', 'description_tokens']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from unidecode import unidecode\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "NOTEBOOK_DIR = Path(__file__).resolve().parent if \"__file__\" in globals() else Path().resolve()\n",
    "REPO_ROOT = NOTEBOOK_DIR.parents[1] if NOTEBOOK_DIR.name == \"part_1\" else NOTEBOOK_DIR\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "\n",
    "INPUT_FILE = DATA_DIR / \"fashion_products_dataset_preprocessed.json\"  # from Step 1\n",
    "OUTPUT_FILE = DATA_DIR / \"fashion_products_dataset_enriched.json\"\n",
    "\n",
    "print(f\"Using preprocessed dataset: {INPUT_FILE}\")\n",
    "print(f\"Enriched output will be saved to: {OUTPUT_FILE}\")\n",
    "\n",
    "def _ensure_nltk():\n",
    "    try:\n",
    "        stopwords.words(\"english\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"stopwords\")\n",
    "    try:\n",
    "        word_tokenize(\"test\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"punkt\")\n",
    "\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/punkt_tab\")\n",
    "    except LookupError:\n",
    "        try:\n",
    "            nltk.download(\"punkt_tab\")\n",
    "        except Exception:\n",
    "            pass\n",
    "_ensure_nltk()\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "def normalize_basic(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    txt = unidecode(text)\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(r\"http[s]?://\\S+\", \" \", txt)\n",
    "    txt = re.sub(r\"[-_/]\", \" \", txt)\n",
    "    txt = re.sub(r\"[$€£₹¥%]+\", \" \", txt)\n",
    "    txt = re.sub(r\"\\d+\", \" \", txt)\n",
    "    txt = re.sub(r\"[^\\w\\s]\", \" \", txt)\n",
    "    txt = re.sub(r\"[^a-z\\s]\", \" \", txt)\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "    return txt\n",
    "\n",
    "def preprocess_text_field(text: str) -> Dict[str, Any]:\n",
    "    cleaned = normalize_basic(text)\n",
    "    toks = word_tokenize(cleaned) if cleaned else []\n",
    "    toks = [t for t in toks if t not in STOPWORDS]\n",
    "    toks = [STEMMER.stem(t) for t in toks]\n",
    "    return {\"tokens\": toks, \"text\": \" \".join(toks)}\n",
    "\n",
    "def _to_bool(val: Any) -> Optional[bool]:\n",
    "    if isinstance(val, bool):\n",
    "        return val\n",
    "    if val is None:\n",
    "        return None\n",
    "    s = str(val).strip().lower()\n",
    "    if s in {\"true\", \"yes\", \"1\"}:\n",
    "        return True\n",
    "    if s in {\"false\", \"no\", \"0\"}:\n",
    "        return False\n",
    "    return None\n",
    "\n",
    "def _to_float_price(val: Any) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Parses strings like '2,999' or '₹2,999.50' -> 2999.0 / 2999.5\n",
    "    Returns None if cannot parse.\n",
    "    \"\"\"\n",
    "    if val is None:\n",
    "        return None\n",
    "    s = str(val)\n",
    "    s = s.replace(\",\", \"\")\n",
    "    m = re.search(r\"(\\d+(\\.\\d+)?)\", s)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(1))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def _to_int_discount_percent(val: Any) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Parses '69% off' -> 69; '15%' -> 15\n",
    "    \"\"\"\n",
    "    if val is None:\n",
    "        return None\n",
    "    m = re.search(r\"(\\d+)\\s*%?\", str(val))\n",
    "    try:\n",
    "        return int(m.group(1)) if m else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _to_float_rating(val: Any) -> Optional[float]:\n",
    "    if val is None:\n",
    "        return None\n",
    "    try:\n",
    "        return float(str(val).strip())\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def _norm_str(val: Any) -> Optional[str]:\n",
    "    if val is None:\n",
    "        return None\n",
    "    s = unidecode(str(val)).strip().lower()\n",
    "    return s if s else None\n",
    "\n",
    "def flatten_product_details(pd: Any) -> Tuple[Dict[str, str], str]:\n",
    "    \"\"\"\n",
    "    Converts product_details list of dicts into:\n",
    "      - a single dict {key: value, ...} (lowercased)\n",
    "      - a single text string \"key: value; key2: value2\"\n",
    "    \"\"\"\n",
    "    out: Dict[str, str] = {}\n",
    "    if isinstance(pd, list):\n",
    "        for item in pd:\n",
    "            if isinstance(item, dict):\n",
    "                for k, v in item.items():\n",
    "                    k_norm = _norm_str(k) or \"\"\n",
    "                    v_norm = _norm_str(v) or \"\"\n",
    "                    if k_norm:\n",
    "                        out[k_norm] = v_norm\n",
    "    elif isinstance(pd, dict):\n",
    "        for k, v in pd.items():\n",
    "            k_norm = _norm_str(k) or \"\"\n",
    "            v_norm = _norm_str(v) or \"\"\n",
    "            if k_norm:\n",
    "                out[k_norm] = v_norm\n",
    "    # Build a readable text\n",
    "    pd_text = \"; \".join([f\"{k}: {v}\" if v else f\"{k}\" for k, v in out.items()]).strip()\n",
    "    return out, pd_text\n",
    "\n",
    "def bucket_price(x: Optional[float]) -> Optional[str]:\n",
    "    if x is None:\n",
    "        return None\n",
    "    if x < 1000: return \"low\"\n",
    "    if x < 3000: return \"mid\"\n",
    "    return \"high\"\n",
    "\n",
    "def bucket_discount(p: Optional[int]) -> Optional[str]:\n",
    "    if p is None: return None\n",
    "    if p == 0: return \"0\"\n",
    "    if p <= 20: return \"1-20\"\n",
    "    if p <= 40: return \"21-40\"\n",
    "    return \"41+\"\n",
    "\n",
    "def bucket_rating(r: Optional[float]) -> Optional[str]:\n",
    "    if r is None: return None\n",
    "    if r < 2: return \"0-2\"\n",
    "    if r < 3.5: return \"2-3.5\"\n",
    "    if r < 4.5: return \"3.5-4.5\"\n",
    "    return \"4.5-5\"\n",
    "\n",
    "def enrich_record(rec: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    out = dict(rec)  # keep everything for later output\n",
    "\n",
    "    # Normalize categorical fields for faceting\n",
    "    brand = _norm_str(rec.get(\"brand\"))\n",
    "    category = _norm_str(rec.get(\"category\"))\n",
    "    sub_category = _norm_str(rec.get(\"sub_category\"))\n",
    "    seller = _norm_str(rec.get(\"seller\"))\n",
    "    out[\"brand_norm\"] = brand\n",
    "    out[\"category_norm\"] = category\n",
    "    out[\"sub_category_norm\"] = sub_category\n",
    "    out[\"seller_norm\"] = seller\n",
    "\n",
    "    # product_details flattening\n",
    "    pd_map, pd_text = flatten_product_details(rec.get(\"product_details\"))\n",
    "    out[\"product_details_map\"] = pd_map\n",
    "    out[\"product_details_text\"] = pd_text\n",
    "\n",
    "    # Build one merged \"metadata text\" field for recall\n",
    "    parts = [\n",
    "        brand or \"\",\n",
    "        category or \"\",\n",
    "        sub_category or \"\",\n",
    "        seller or \"\",\n",
    "        pd_text or \"\",\n",
    "    ]\n",
    "    metadata_text_raw = \" | \".join([p for p in parts if p])\n",
    "    meta_proc = preprocess_text_field(metadata_text_raw)\n",
    "    out[\"metadata_tokens\"] = meta_proc[\"tokens\"]\n",
    "    out[\"metadata_clean\"] = meta_proc[\"text\"]\n",
    "\n",
    "    # Numeric/bool parsing (for filters/ranking features)\n",
    "    out_of_stock = _to_bool(rec.get(\"out_of_stock\"))\n",
    "    actual_price_num = _to_float_price(rec.get(\"actual_price\"))\n",
    "    selling_price_num = _to_float_price(rec.get(\"selling_price\"))\n",
    "    discount_pct = _to_int_discount_percent(rec.get(\"discount\"))\n",
    "    average_rating_num = _to_float_rating(rec.get(\"average_rating\"))\n",
    "\n",
    "    out[\"out_of_stock_bool\"] = out_of_stock\n",
    "    out[\"actual_price_num\"] = actual_price_num\n",
    "    out[\"selling_price_num\"] = selling_price_num\n",
    "    out[\"discount_pct\"] = discount_pct\n",
    "    out[\"average_rating_num\"] = average_rating_num\n",
    "\n",
    "    # Buckets for UX/boosting/facets\n",
    "    out[\"price_bucket\"] = bucket_price(selling_price_num or actual_price_num)\n",
    "    out[\"discount_bucket\"] = bucket_discount(discount_pct)\n",
    "    out[\"rating_bucket\"] = bucket_rating(average_rating_num)\n",
    "\n",
    "    return out\n",
    "\n",
    "def read_json(path: Path):\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if not isinstance(data, list):\n",
    "        raise ValueError(\"Expected a JSON array.\")\n",
    "    return data\n",
    "\n",
    "def write_json(path: Path, items):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(items, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "if not INPUT_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Input file not found: {INPUT_FILE}\")\n",
    "\n",
    "data = read_json(INPUT_FILE)\n",
    "enriched = [enrich_record(rec) for rec in data]\n",
    "write_json(OUTPUT_FILE, enriched)\n",
    "\n",
    "print(f\"Enriched {len(enriched)} records.\")\n",
    "print(f\"Saved to {OUTPUT_FILE}\")\n",
    "print(\"Example (keys excerpt):\", list(enriched[0].keys())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9702f6",
   "metadata": {},
   "source": [
    "## **PART 2: EDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508249f2",
   "metadata": {},
   "source": [
    "### **Main Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32521cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/fashion_products_dataset_enriched.json\n",
      "Saving outputs to: /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/eda_outputs\n",
      "Total products: 28080\n",
      "Wrote summary: /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/eda_outputs/summary.txt\n",
      "[INFO] spaCy NER skipped (model not installed). To enable: pip install spacy && python -m spacy download en_core_web_sm\n",
      "\n",
      "=== KPI SUMMARY ===\n",
      "n_products: 28080\n",
      "vocab_size_title_desc_meta: 6650\n",
      "median_title_len: 6.0\n",
      "median_desc_len: 9.0\n",
      "median_price: 545.0\n",
      "median_rating: 3.8\n",
      "median_discount_pct: 53.0\n",
      "out_of_stock_ratio: 0.05854700854700855\n",
      "\n",
      "Done. Check the 'outputs' folder for figures, CSVs, and summaries.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "\n",
    "OTEBOOK_DIR = Path().resolve()\n",
    "REPO_ROOT = NOTEBOOK_DIR.parents[1]\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "INPUT = DATA_DIR / \"fashion_products_dataset_enriched.json\"\n",
    "\n",
    "OUT_DIR = DATA_DIR / \"eda_outputs\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Reading: {INPUT}\")\n",
    "print(f\"Saving outputs to: {OUT_DIR}\")\n",
    "\n",
    "\n",
    "\n",
    "if not INPUT.exists():\n",
    "    raise FileNotFoundError(f\"Input not found: {INPUT}\")\n",
    "\n",
    "df = pd.read_json(INPUT)\n",
    "n_docs = len(df)\n",
    "print(\"Total products:\", n_docs)\n",
    "\n",
    "def to_num(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "\n",
    "summary_txt = OUT_DIR / \"summary.txt\"\n",
    "with summary_txt.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"Total products: {n_docs}\\n\\n\")\n",
    "    f.write(\"Missing values (top 20 columns):\\n\")\n",
    "    f.write(df.isna().sum().sort_values(ascending=False).head(20).to_string())\n",
    "    f.write(\"\\n\\nColumns:\\n\")\n",
    "    f.write(\", \".join(df.columns))\n",
    "print(f\"Wrote summary: {summary_txt}\")\n",
    "\n",
    "\n",
    "# Text stats: word counts & vocab\n",
    "def word_count_col(series: pd.Series) -> pd.Series:\n",
    "    return series.fillna(\"\").astype(str).str.split().apply(len)\n",
    "\n",
    "df[\"title_word_count\"] = word_count_col(df.get(\"title_clean\", pd.Series()))\n",
    "df[\"desc_word_count\"] = word_count_col(df.get(\"description_clean\", pd.Series()))\n",
    "\n",
    "# Hist: title length\n",
    "plt.figure()\n",
    "df[\"title_word_count\"].dropna().plot(kind=\"hist\", bins=30)\n",
    "plt.title(\"Title word count distribution\")\n",
    "plt.xlabel(\"Words in title\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"hist_title_word_count.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Hist: description length\n",
    "plt.figure()\n",
    "df[\"desc_word_count\"].dropna().plot(kind=\"hist\", bins=30)\n",
    "plt.title(\"Description word count distribution\")\n",
    "plt.xlabel(\"Words in description\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"hist_desc_word_count.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Vocabulary size (title + description + metadata)\n",
    "def concat_text(cols):\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    if not cols:\n",
    "        return \"\"\n",
    "    return \" \".join(df[c].dropna().astype(str) for c in cols)\n",
    "\n",
    "# Prefer *_clean fields if present\n",
    "text_sources = []\n",
    "for col in [\"title_clean\", \"description_clean\", \"metadata_clean\"]:\n",
    "    if col in df.columns:\n",
    "        text_sources.append(col)\n",
    "\n",
    "all_text = \" \".join(df[col].dropna().astype(str).tolist()) if text_sources else \"\"\n",
    "vocab_size = len(set(all_text.split())) if all_text else 0\n",
    "\n",
    "with summary_txt.open(\"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"\\nVocabulary size (title/desc/metadata): {vocab_size}\\n\")\n",
    "\n",
    "\n",
    "# Word clouds\n",
    "def make_wordcloud(text_series: pd.Series, out_name: str, title: str):\n",
    "    text = \" \".join(text_series.dropna().astype(str).tolist())\n",
    "    if not text.strip():\n",
    "        print(f\"[WARN] No text for wordcloud: {out_name}\")\n",
    "        return\n",
    "    wc = WordCloud(width=1200, height=600, background_color=\"white\").generate(text)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / out_name, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "if \"title_clean\" in df.columns:\n",
    "    make_wordcloud(df[\"title_clean\"], \"wc_title.png\", \"Word Cloud — Titles\")\n",
    "\n",
    "if \"description_clean\" in df.columns:\n",
    "    make_wordcloud(df[\"description_clean\"], \"wc_description.png\", \"Word Cloud — Descriptions\")\n",
    "\n",
    "if \"metadata_clean\" in df.columns:\n",
    "    make_wordcloud(df[\"metadata_clean\"], \"wc_metadata.png\", \"Word Cloud — Metadata\")\n",
    "\n",
    "\n",
    "# Numeric distributions: rating, price, discount\n",
    "if \"average_rating_num\" in df.columns:\n",
    "    plt.figure()\n",
    "    to_num(df[\"average_rating_num\"]).dropna().plot(kind=\"hist\", bins=25)\n",
    "    plt.title(\"Distribution of average ratings\")\n",
    "    plt.xlabel(\"Rating\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"hist_ratings.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "if \"selling_price_num\" in df.columns:\n",
    "    plt.figure()\n",
    "    to_num(df[\"selling_price_num\"]).dropna().plot(kind=\"hist\", bins=40)\n",
    "    plt.title(\"Distribution of selling prices\")\n",
    "    plt.xlabel(\"Price\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"hist_prices.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "if \"discount_pct\" in df.columns:\n",
    "    plt.figure()\n",
    "    to_num(df[\"discount_pct\"]).dropna().plot(kind=\"hist\", bins=40)\n",
    "    plt.title(\"Distribution of discounts (%)\")\n",
    "    plt.xlabel(\"Discount %\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"hist_discounts.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Out-of-stock distribution\n",
    "if \"out_of_stock_bool\" in df.columns:\n",
    "    plt.figure()\n",
    "    df[\"out_of_stock_bool\"].value_counts(dropna=False).plot(kind=\"bar\")\n",
    "    plt.title(\"Out-of-stock distribution\")\n",
    "    plt.xlabel(\"Out of stock\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"bar_out_of_stock.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# Top brands / sellers\n",
    "def bar_top_counts(series: pd.Series, out_name: str, title: str, topn: int = 15):\n",
    "    vc = series.dropna().astype(str).str.strip().replace({\"\": None}).dropna().value_counts().head(topn)\n",
    "    if vc.empty:\n",
    "        print(f\"[WARN] No data for {title}\")\n",
    "        return\n",
    "    plt.figure()\n",
    "    vc[::-1].plot(kind=\"barh\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Count\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / out_name, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "if \"brand_norm\" in df.columns:\n",
    "    bar_top_counts(df[\"brand_norm\"], \"bar_top_brands.png\", \"Top brands (count)\")\n",
    "\n",
    "if \"seller_norm\" in df.columns:\n",
    "    bar_top_counts(df[\"seller_norm\"], \"bar_top_sellers.png\", \"Top sellers (count)\")\n",
    "\n",
    "\n",
    "# Rankings: top rated / cheapest / top discounted\n",
    "def save_table(df_sorted: pd.DataFrame, cols, out_csv):\n",
    "    cols = [c for c in cols if c in df_sorted.columns]\n",
    "    if not cols:\n",
    "        return\n",
    "    df_sorted[cols].to_csv(OUT_DIR / out_csv, index=False)\n",
    "\n",
    "# Top rated\n",
    "if \"average_rating_num\" in df.columns:\n",
    "    top_rated = df.sort_values([\"average_rating_num\", \"selling_price_num\"], ascending=[False, True]).head(50)\n",
    "    save_table(\n",
    "        top_rated,\n",
    "        [\"pid\", \"title\", \"brand\", \"category\", \"sub_category\", \"average_rating_num\", \"selling_price_num\", \"discount_pct\", \"url\"],\n",
    "        \"top_rated_products.csv\"\n",
    "    )\n",
    "\n",
    "# Cheapest (by selling price if available, fallback to actual price)\n",
    "price_col = \"selling_price_num\" if \"selling_price_num\" in df.columns else (\"actual_price_num\" if \"actual_price_num\" in df.columns else None)\n",
    "if price_col:\n",
    "    cheapest = df[df[price_col].notna()].sort_values(price_col, ascending=True).head(50)\n",
    "    save_table(\n",
    "        cheapest,\n",
    "        [\"pid\", \"title\", \"brand\", \"category\", \"sub_category\", price_col, \"average_rating_num\", \"discount_pct\", \"url\"],\n",
    "        \"cheapest_products.csv\"\n",
    "    )\n",
    "\n",
    "# Top discounted\n",
    "if \"discount_pct\" in df.columns:\n",
    "    top_disc = df[df[\"discount_pct\"].notna()].sort_values(\"discount_pct\", ascending=False).head(50)\n",
    "    save_table(\n",
    "        top_disc,\n",
    "        [\"pid\", \"title\", \"brand\", \"category\", \"sub_category\", \"discount_pct\", \"selling_price_num\", \"average_rating_num\", \"url\"],\n",
    "        \"top_discounted_products.csv\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Optional: Named Entity Recognition (spaCy)\n",
    "def try_ner_sample(df_in: pd.DataFrame, n_docs: int = 20):\n",
    "    try:\n",
    "        import spacy\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    except Exception as e:\n",
    "        print(\"[INFO] spaCy NER skipped (model not installed). To enable: pip install spacy && python -m spacy download en_core_web_sm\")\n",
    "        return\n",
    "\n",
    "    sample = df_in[\"description\"].dropna().astype(str).sample(min(n_docs, len(df_in)), random_state=42)\n",
    "    ents_rows = []\n",
    "    for text in sample:\n",
    "        doc = nlp(text[:2000])  # limit very long descriptions\n",
    "        for ent in doc.ents:\n",
    "            ents_rows.append({\"text\": text[:80] + (\"...\" if len(text) > 80 else \"\"),\n",
    "                              \"entity\": ent.text, \"label\": ent.label_})\n",
    "    if ents_rows:\n",
    "        ner_df = pd.DataFrame(ents_rows)\n",
    "        ner_csv = OUT_DIR / \"ner_sample_entities.csv\"\n",
    "        ner_df.to_csv(ner_csv, index=False)\n",
    "        print(f\"Saved NER sample entities: {ner_csv}\")\n",
    "\n",
    "try_ner_sample(df, n_docs=20)\n",
    "\n",
    "\n",
    "# Print quick KPI summary\n",
    "def safe_median(series):\n",
    "    s = pd.to_numeric(series, errors=\"coerce\").dropna()\n",
    "    return float(s.median()) if not s.empty else None\n",
    "\n",
    "kpis = {\n",
    "    \"n_products\": n_docs,\n",
    "    \"vocab_size_title_desc_meta\": vocab_size,\n",
    "    \"median_title_len\": float(df[\"title_word_count\"].median()) if \"title_word_count\" in df else None,\n",
    "    \"median_desc_len\": float(df[\"desc_word_count\"].median()) if \"desc_word_count\" in df else None,\n",
    "    \"median_price\": safe_median(df.get(\"selling_price_num\", pd.Series())),\n",
    "    \"median_rating\": safe_median(df.get(\"average_rating_num\", pd.Series())),\n",
    "    \"median_discount_pct\": safe_median(df.get(\"discount_pct\", pd.Series())),\n",
    "    \"out_of_stock_ratio\": float(df[\"out_of_stock_bool\"].mean()) if \"out_of_stock_bool\" in df and df[\"out_of_stock_bool\"].notna().any() else None,\n",
    "}\n",
    "\n",
    "kpi_lines = \"\\n\".join([f\"{k}: {v}\" for k, v in kpis.items()])\n",
    "print(\"\\n=== KPI SUMMARY ===\")\n",
    "print(kpi_lines)\n",
    "with (OUT_DIR / \"kpi_summary.txt\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(kpi_lines)\n",
    "\n",
    "print(\"\\nDone. Check the 'outputs' folder for figures, CSVs, and summaries.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irwa_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
