{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da66747f",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# DELIVERY 1  \n",
    "## **TEXT PROCESSING & EXPLORATORY DATA ANALYSIS (EDA)**\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bec7e2",
   "metadata": {},
   "source": [
    "## PART 1: **DATA PREPARATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bac50d9",
   "metadata": {},
   "source": [
    "### STEP 1 — **Preprocessing Pipeline (NLTK)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c4f09",
   "metadata": {},
   "source": [
    "### **Main Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c8b95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset: /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/fashion_products_dataset.json\n",
      "Output will be saved to: /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/fashion_products_dataset_preprocessed.json\n",
      "Processed 28080 records.\n",
      "Saved to /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/fashion_products_dataset_preprocessed.json\n",
      "Example output:\n",
      "\n",
      "{\n",
      "  \"_id\": \"fa8e22d6-c0b6-5229-bb9e-ad52eda39a0a\",\n",
      "  \"actual_price\": \"2,999\",\n",
      "  \"average_rating\": \"3.9\",\n",
      "  \"brand\": \"York\",\n",
      "  \"category\": \"Clothing and Accessories\",\n",
      "  \"crawled_at\": 1612987911000,\n",
      "  \"description\": \"Yorker trackpants made from 100% rich combed cotton giving it a rich look.Designed for Comfort,Skin friendly fabric,itch-free waistband & great for all year round use Proudly made in India\",\n",
      "  \"discount\": \"69% off\",\n",
      "  \"images\": [\n",
      "    \"https://rukminim1.flixcart.com/image/128/128/jr3t5e80/track-pant/z/y/n/m-1005combo2-yorker-original-imafczg3xfh5qqd4.jpeg?q=70\",\n",
      "    \"https://rukminim1.flixcart.com/image/128/128/jr58l8w0/track-pant/w/d/a/l-1005combo8-yorker-original-imafczg3pgtxgraq.jpeg?q=70\"\n",
      "  ],\n",
      "  \"out_of_stock\": false,\n",
      "  \"pid\": \"TKPFCZ9EA7H5FYZH\",\n",
      "  \"product_details\": [\n",
      "    {\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from unidecode import unidecode\n",
    "\n",
    "NOTEBOOK_DIR = Path(__file__).resolve().parent if \"__file__\" in globals() else Path().resolve()\n",
    "REPO_ROOT = NOTEBOOK_DIR.parents[1] if NOTEBOOK_DIR.name == \"part_1\" else NOTEBOOK_DIR\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "\n",
    "INPUT_FILE = DATA_DIR / \"fashion_products_dataset.json\"\n",
    "OUTPUT_FILE = DATA_DIR / \"fashion_products_dataset_preprocessed.json\"\n",
    "\n",
    "print(f\"Using dataset: {INPUT_FILE}\")\n",
    "print(f\"Output will be saved to: {OUTPUT_FILE}\")\n",
    "\n",
    "def _ensure_nltk():\n",
    "    try:\n",
    "        stopwords.words(\"english\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"stopwords\")\n",
    "    try:\n",
    "        word_tokenize(\"test\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"punkt\")\n",
    "_ensure_nltk()\n",
    "\n",
    "\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "\n",
    "def normalize_basic(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Basic normalization:\n",
    "      - Lowercase\n",
    "      - Remove accents with unidecode\n",
    "      - Remove currency symbols and numbers\n",
    "      - Remove punctuation marks\n",
    "      - Replace dashes/underscores with space\n",
    "      - Collapse extra whitespace\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    txt = unidecode(text)\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(r\"http[s]?://\\S+\", \" \", txt)\n",
    "    txt = re.sub(r\"[-_/]\", \" \", txt)\n",
    "    txt = re.sub(r\"[$€£₹¥%]+\", \" \", txt)\n",
    "    txt = re.sub(r\"\\d+\", \" \", txt)\n",
    "    txt = re.sub(r\"[^\\w\\s]\", \" \", txt)\n",
    "    txt = re.sub(r\"[^a-z\\s]\", \" \", txt)\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "    return txt\n",
    "\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return word_tokenize(text) if text else []\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens: List[str]) -> List[str]:\n",
    "    return [t for t in tokens if t not in STOPWORDS]\n",
    "\n",
    "\n",
    "def stem(tokens: List[str]) -> List[str]:\n",
    "    return [STEMMER.stem(t) for t in tokens]\n",
    "\n",
    "\n",
    "def preprocess_text_field(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Apply the full preprocessing pipeline to one text field.\"\"\"\n",
    "    cleaned = normalize_basic(text)\n",
    "    toks = tokenize(cleaned)\n",
    "    toks_nostop = remove_stopwords(toks)\n",
    "    toks_stem = stem(toks_nostop)\n",
    "    return {\n",
    "        \"tokens\": toks_stem,\n",
    "        \"text\": \" \".join(toks_stem)\n",
    "    }\n",
    "\n",
    "\n",
    "def process_record(rec: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Process one record and add preprocessed fields.\"\"\"\n",
    "    title = rec.get(\"title\", \"\")\n",
    "    desc = rec.get(\"description\", \"\")\n",
    "\n",
    "    t_proc = preprocess_text_field(title)\n",
    "    d_proc = preprocess_text_field(desc)\n",
    "\n",
    "    rec_out = dict(rec)\n",
    "    rec_out[\"title_tokens\"] = t_proc[\"tokens\"]\n",
    "    rec_out[\"title_clean\"] = t_proc[\"text\"]\n",
    "    rec_out[\"description_tokens\"] = d_proc[\"tokens\"]\n",
    "    rec_out[\"description_clean\"] = d_proc[\"text\"]\n",
    "    return rec_out\n",
    "\n",
    "\n",
    "def read_json(path: Path):\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        if not isinstance(data, list):\n",
    "            raise ValueError(\"Expected a JSON array.\")\n",
    "        return data\n",
    "\n",
    "\n",
    "def write_json(path: Path, items):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(items, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "if not INPUT_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Input file not found: {INPUT_FILE}\")\n",
    "\n",
    "data = read_json(INPUT_FILE)\n",
    "processed = [process_record(rec) for rec in data]\n",
    "write_json(OUTPUT_FILE, processed)\n",
    "\n",
    "print(f\"Processed {len(processed)} records.\")\n",
    "print(f\"Saved to {OUTPUT_FILE}\")\n",
    "print(\"Example output:\\n\")\n",
    "print(json.dumps(processed[0], indent=2)[:800])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559f932e",
   "metadata": {},
   "source": [
    "### **Verification Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24520638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing fields in first 200 records: [] (showing up to 10)\n",
      "\n",
      "TITLE RAW:  Solid Women Multicolor Track Pants\n",
      "TITLE CLEAN: solid women multicolor track pant\n",
      "TITLE TOKENS: ['solid', 'women', 'multicolor', 'track', 'pant']\n",
      "DESC CLEAN: yorker trackpant made rich comb cotton give rich look design comfort skin friendli fabric itch free waistband great year ...\n",
      "\n",
      "TITLE RAW:  Solid Men Blue Track Pants\n",
      "TITLE CLEAN: solid men blue track pant\n",
      "TITLE TOKENS: ['solid', 'men', 'blue', 'track', 'pant']\n",
      "DESC CLEAN: yorker trackpant made rich comb cotton give rich look design comfort skin friendli fabric itch free waistband great year ...\n",
      "\n",
      "TITLE RAW:  Solid Men Multicolor Track Pants\n",
      "TITLE CLEAN: solid men multicolor track pant\n",
      "TITLE TOKENS: ['solid', 'men', 'multicolor', 'track', 'pant']\n",
      "DESC CLEAN: yorker trackpant made rich comb cotton give rich look design comfort skin friendli fabric itch free waistband great year ...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, itertools\n",
    "\n",
    "out_path = Path(\"../..\") / \"data\" / \"fashion_products_dataset_preprocessed.json\"  # adjust if needed\n",
    "data = json.loads(Path(out_path).read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# 1) Confirm new keys exist\n",
    "required_keys = {\"title_tokens\",\"title_clean\",\"description_tokens\",\"description_clean\"}\n",
    "missing = [i for i,r in enumerate(data[:200]) if not required_keys.issubset(r)]\n",
    "print(\"Missing fields in first 200 records:\", missing[:10], \"(showing up to 10)\")\n",
    "\n",
    "# 2) Spot-check a few examples (before vs after)\n",
    "for rec in itertools.islice((r for r in data if r.get(\"title\")), 3):\n",
    "    print(\"\\nTITLE RAW: \", rec[\"title\"])\n",
    "    print(\"TITLE CLEAN:\", rec[\"title_clean\"])\n",
    "    print(\"TITLE TOKENS:\", rec[\"title_tokens\"][:15])\n",
    "    print(\"DESC CLEAN:\", rec[\"description_clean\"][:120], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68afb3cb",
   "metadata": {},
   "source": [
    "### STEP 2 — **Non-Text Fields Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f0172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using preprocessed dataset: /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/fashion_products_dataset_preprocessed.json\n",
      "Enriched output will be saved to: /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/fashion_products_dataset_enriched.json\n",
      "Enriched 28080 records.\n",
      "Saved to /Users/pauchaves/Documents/Mathematical Engineering in Data Science/4th Year/1st Trimester/WEB RETRIEVAL/irwa-search-engine/data/fashion_products_dataset_enriched.json\n",
      "Example (keys excerpt): ['_id', 'actual_price', 'average_rating', 'brand', 'category', 'crawled_at', 'description', 'discount', 'images', 'out_of_stock', 'pid', 'product_details', 'seller', 'selling_price', 'sub_category', 'title', 'url', 'title_tokens', 'title_clean', 'description_tokens']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from unidecode import unidecode\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "NOTEBOOK_DIR = Path(__file__).resolve().parent if \"__file__\" in globals() else Path().resolve()\n",
    "REPO_ROOT = NOTEBOOK_DIR.parents[1] if NOTEBOOK_DIR.name == \"part_1\" else NOTEBOOK_DIR\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "\n",
    "INPUT_FILE = DATA_DIR / \"fashion_products_dataset_preprocessed.json\"  # from Step 1\n",
    "OUTPUT_FILE = DATA_DIR / \"fashion_products_dataset_enriched.json\"\n",
    "\n",
    "print(f\"Using preprocessed dataset: {INPUT_FILE}\")\n",
    "print(f\"Enriched output will be saved to: {OUTPUT_FILE}\")\n",
    "\n",
    "def _ensure_nltk():\n",
    "    try:\n",
    "        stopwords.words(\"english\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"stopwords\")\n",
    "    try:\n",
    "        word_tokenize(\"test\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"punkt\")\n",
    "\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/punkt_tab\")\n",
    "    except LookupError:\n",
    "        try:\n",
    "            nltk.download(\"punkt_tab\")\n",
    "        except Exception:\n",
    "            pass\n",
    "_ensure_nltk()\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "def normalize_basic(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    txt = unidecode(text)\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(r\"http[s]?://\\S+\", \" \", txt)\n",
    "    txt = re.sub(r\"[-_/]\", \" \", txt)\n",
    "    txt = re.sub(r\"[$€£₹¥%]+\", \" \", txt)\n",
    "    txt = re.sub(r\"\\d+\", \" \", txt)\n",
    "    txt = re.sub(r\"[^\\w\\s]\", \" \", txt)\n",
    "    txt = re.sub(r\"[^a-z\\s]\", \" \", txt)\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "    return txt\n",
    "\n",
    "def preprocess_text_field(text: str) -> Dict[str, Any]:\n",
    "    cleaned = normalize_basic(text)\n",
    "    toks = word_tokenize(cleaned) if cleaned else []\n",
    "    toks = [t for t in toks if t not in STOPWORDS]\n",
    "    toks = [STEMMER.stem(t) for t in toks]\n",
    "    return {\"tokens\": toks, \"text\": \" \".join(toks)}\n",
    "\n",
    "def _to_bool(val: Any) -> Optional[bool]:\n",
    "    if isinstance(val, bool):\n",
    "        return val\n",
    "    if val is None:\n",
    "        return None\n",
    "    s = str(val).strip().lower()\n",
    "    if s in {\"true\", \"yes\", \"1\"}:\n",
    "        return True\n",
    "    if s in {\"false\", \"no\", \"0\"}:\n",
    "        return False\n",
    "    return None\n",
    "\n",
    "def _to_float_price(val: Any) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Parses strings like '2,999' or '₹2,999.50' -> 2999.0 / 2999.5\n",
    "    Returns None if cannot parse.\n",
    "    \"\"\"\n",
    "    if val is None:\n",
    "        return None\n",
    "    s = str(val)\n",
    "    s = s.replace(\",\", \"\")\n",
    "    m = re.search(r\"(\\d+(\\.\\d+)?)\", s)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(1))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def _to_int_discount_percent(val: Any) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Parses '69% off' -> 69; '15%' -> 15\n",
    "    \"\"\"\n",
    "    if val is None:\n",
    "        return None\n",
    "    m = re.search(r\"(\\d+)\\s*%?\", str(val))\n",
    "    try:\n",
    "        return int(m.group(1)) if m else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _to_float_rating(val: Any) -> Optional[float]:\n",
    "    if val is None:\n",
    "        return None\n",
    "    try:\n",
    "        return float(str(val).strip())\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def _norm_str(val: Any) -> Optional[str]:\n",
    "    if val is None:\n",
    "        return None\n",
    "    s = unidecode(str(val)).strip().lower()\n",
    "    return s if s else None\n",
    "\n",
    "def flatten_product_details(pd: Any) -> Tuple[Dict[str, str], str]:\n",
    "    \"\"\"\n",
    "    Converts product_details list of dicts into:\n",
    "      - a single dict {key: value, ...} (lowercased)\n",
    "      - a single text string \"key: value; key2: value2\"\n",
    "    \"\"\"\n",
    "    out: Dict[str, str] = {}\n",
    "    if isinstance(pd, list):\n",
    "        for item in pd:\n",
    "            if isinstance(item, dict):\n",
    "                for k, v in item.items():\n",
    "                    k_norm = _norm_str(k) or \"\"\n",
    "                    v_norm = _norm_str(v) or \"\"\n",
    "                    if k_norm:\n",
    "                        out[k_norm] = v_norm\n",
    "    elif isinstance(pd, dict):\n",
    "        for k, v in pd.items():\n",
    "            k_norm = _norm_str(k) or \"\"\n",
    "            v_norm = _norm_str(v) or \"\"\n",
    "            if k_norm:\n",
    "                out[k_norm] = v_norm\n",
    "    # Build a readable text\n",
    "    pd_text = \"; \".join([f\"{k}: {v}\" if v else f\"{k}\" for k, v in out.items()]).strip()\n",
    "    return out, pd_text\n",
    "\n",
    "def bucket_price(x: Optional[float]) -> Optional[str]:\n",
    "    if x is None:\n",
    "        return None\n",
    "    if x < 1000: return \"low\"\n",
    "    if x < 3000: return \"mid\"\n",
    "    return \"high\"\n",
    "\n",
    "def bucket_discount(p: Optional[int]) -> Optional[str]:\n",
    "    if p is None: return None\n",
    "    if p == 0: return \"0\"\n",
    "    if p <= 20: return \"1-20\"\n",
    "    if p <= 40: return \"21-40\"\n",
    "    return \"41+\"\n",
    "\n",
    "def bucket_rating(r: Optional[float]) -> Optional[str]:\n",
    "    if r is None: return None\n",
    "    if r < 2: return \"0-2\"\n",
    "    if r < 3.5: return \"2-3.5\"\n",
    "    if r < 4.5: return \"3.5-4.5\"\n",
    "    return \"4.5-5\"\n",
    "\n",
    "def enrich_record(rec: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    out = dict(rec)  # keep everything for later output\n",
    "\n",
    "    # Normalize categorical fields for faceting\n",
    "    brand = _norm_str(rec.get(\"brand\"))\n",
    "    category = _norm_str(rec.get(\"category\"))\n",
    "    sub_category = _norm_str(rec.get(\"sub_category\"))\n",
    "    seller = _norm_str(rec.get(\"seller\"))\n",
    "    out[\"brand_norm\"] = brand\n",
    "    out[\"category_norm\"] = category\n",
    "    out[\"sub_category_norm\"] = sub_category\n",
    "    out[\"seller_norm\"] = seller\n",
    "\n",
    "    # product_details flattening\n",
    "    pd_map, pd_text = flatten_product_details(rec.get(\"product_details\"))\n",
    "    out[\"product_details_map\"] = pd_map\n",
    "    out[\"product_details_text\"] = pd_text\n",
    "\n",
    "    # Build one merged \"metadata text\" field for recall\n",
    "    parts = [\n",
    "        brand or \"\",\n",
    "        category or \"\",\n",
    "        sub_category or \"\",\n",
    "        seller or \"\",\n",
    "        pd_text or \"\",\n",
    "    ]\n",
    "    metadata_text_raw = \" | \".join([p for p in parts if p])\n",
    "    meta_proc = preprocess_text_field(metadata_text_raw)\n",
    "    out[\"metadata_tokens\"] = meta_proc[\"tokens\"]\n",
    "    out[\"metadata_clean\"] = meta_proc[\"text\"]\n",
    "\n",
    "    # Numeric/bool parsing (for filters/ranking features)\n",
    "    out_of_stock = _to_bool(rec.get(\"out_of_stock\"))\n",
    "    actual_price_num = _to_float_price(rec.get(\"actual_price\"))\n",
    "    selling_price_num = _to_float_price(rec.get(\"selling_price\"))\n",
    "    discount_pct = _to_int_discount_percent(rec.get(\"discount\"))\n",
    "    average_rating_num = _to_float_rating(rec.get(\"average_rating\"))\n",
    "\n",
    "    out[\"out_of_stock_bool\"] = out_of_stock\n",
    "    out[\"actual_price_num\"] = actual_price_num\n",
    "    out[\"selling_price_num\"] = selling_price_num\n",
    "    out[\"discount_pct\"] = discount_pct\n",
    "    out[\"average_rating_num\"] = average_rating_num\n",
    "\n",
    "    # Buckets for UX/boosting/facets\n",
    "    out[\"price_bucket\"] = bucket_price(selling_price_num or actual_price_num)\n",
    "    out[\"discount_bucket\"] = bucket_discount(discount_pct)\n",
    "    out[\"rating_bucket\"] = bucket_rating(average_rating_num)\n",
    "\n",
    "    return out\n",
    "\n",
    "def read_json(path: Path):\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    if not isinstance(data, list):\n",
    "        raise ValueError(\"Expected a JSON array.\")\n",
    "    return data\n",
    "\n",
    "def write_json(path: Path, items):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(items, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "if not INPUT_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Input file not found: {INPUT_FILE}\")\n",
    "\n",
    "data = read_json(INPUT_FILE)\n",
    "enriched = [enrich_record(rec) for rec in data]\n",
    "write_json(OUTPUT_FILE, enriched)\n",
    "\n",
    "print(f\"Enriched {len(enriched)} records.\")\n",
    "print(f\"Saved to {OUTPUT_FILE}\")\n",
    "print(\"Example (keys excerpt):\", list(enriched[0].keys())[:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irwa_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
